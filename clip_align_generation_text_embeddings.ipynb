{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["VExpsXZhVlwz","1luWablvWWyv","yGpkPnme_ob8"],"authorship_tag":"ABX9TyOgB8Flnv8xfJhYxI/M41kb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Notebook for generating the textual embeddings necessary for the labeled training dataset creation for the ML based approaches.\n","\n","Choice whether to use CLIP or ALIGN - through \"MODEL\" parameter in Setup section"],"metadata":{"id":"7n4JDnLM5n2j"}},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"p6HYfXuTVjVv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j5xn_ahXN9cR"},"outputs":[],"source":["# choose which model to use to generate the embeddings: \"CLIP\" or \"ALIGN\"\n","MODEL = 'CLIP'\n","\n","# choose base name where to store embeddings\n","CORRECT_TEXT_EMBEDDINGS_NAME_BASE = f\"text_embeddings_correct_{MODEL.lower()}\"\n","RANDOM_TEXT_EMBEDDINGS_NAME_BASE = f\"text_embeddings_random_{MODEL.lower()}\"\n","\n","# choose dataset\n","DATASET_NAME = 'cats-vs-dogs-large'  # needs to match folder name in FM/datasets\n","LOAD_AND_EMBED_DATASET_IN_BATCHES = True  # True for large datasets, False for small ones\n","USE_CACHED_EMBEDDINGS = f'{MODEL.lower()}_cats-vs-dogs-large.pkl'  # '' for loading the dataset normally, 'CREATE__{x}.pkl' for creating the cache file {x}.pkl, '{x}.pkl' for loading the cache file {x}.pkl\n","MISLABELED_INSTANCES = 'mislabeled_instances_cats-vs-dogs.pkl'  # if not '', but e.g. 'mislabeled_instances_cats-vs-dogs.pkl', the pickle file specifies which files to drop from the loaded embeddings\n","\n","# choose how many image-label mismatches to insert\n","MISMATCH_PORTION = 0.01  # percentage of mismatching image-label pairs added\n","MANIPULATION_TYPES = [0.5, 0.5, 0.0, 0.0]  # how much of the MISMATCH_PORTION to produce by [exchanging images between classes, inserting images from other datasets, inserting randomly generated images, inserting placeholder images]\n","IMAGENET_EMBEDDINGS = f'{MODEL.lower()}_imagenet-subset.pkl'  # specify if MANIPULATION_TYPES[1] > 0\n"]},{"cell_type":"code","source":["root_path = '/content/drive/My Drive/FM/'\n","datasets_path = root_path + 'datasets/'\n","dataset_path = datasets_path + DATASET_NAME + '/'"],"metadata":{"id":"lPLmzPolY4vM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hacky way when hitting \"run all\" that libraries are not reloaded\n","try:\n","  torch.tensor([[0]])\n","  libraries_already_loaded = True\n","except:\n","  libraries_already_loaded = False"],"metadata":{"id":"eX4EXGfbZFKE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if not libraries_already_loaded:\n","  ! pip install ftfy regex tqdm\n","  ! pip install git+https://github.com/openai/CLIP.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UZFiJRzfZHS9","executionInfo":{"status":"ok","timestamp":1707247233403,"user_tz":-60,"elapsed":31690,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"6b513e1e-96f0-4937-90c9-69ad69f1b048"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.3\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-oh1x8e0m\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-oh1x8e0m\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=919ce0f617f6df9a32f71febae854aa7907ae82f7d126e9191250e41d33e92d2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-epstplqc/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]}]},{"cell_type":"code","source":["if not libraries_already_loaded:\n","  import torch\n","  from torchvision import transforms\n","  import clip\n","  from transformers import AutoImageProcessor, ViTModel, AlignProcessor, AlignModel, AutoTokenizer\n","  from transformers.tokenization_utils_base import BatchEncoding\n","  import numpy as np\n","  from matplotlib import pyplot as plt\n","  from PIL import Image\n","  from pkg_resources import packaging\n","  import os\n","  from google.colab import drive\n","  from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n","  import glob\n","  import pickle\n","  from scipy.spatial.distance import cosine\n","\n","print(\"Torch version:\", torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJTdHgqiZH6Z","executionInfo":{"status":"ok","timestamp":1707247252926,"user_tz":-60,"elapsed":19527,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"263435ae-1f3a-4919-8579-04d848cfcff3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Torch version: 2.1.0+cu121\n"]}]},{"cell_type":"code","source":["drive.mount('/content/drive')\n","!ls \"{datasets_path}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TT1T2ceLZe0f","executionInfo":{"status":"ok","timestamp":1707247278535,"user_tz":-60,"elapsed":25637,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"586fbd67-a70f-4125-ca2d-222bcb645461"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","AlexNet_cats-vs-dogs-large.pkl\t\t\t mislabeled_instances_cats-vs-dogs_CATS_ONLY.pkl\n","AlexNet_imagenet-subset.pkl\t\t\t mislabeled_instances_cats-vs-dogs.pkl\n","AlexNet_traffic-signs.pkl\t\t\t note.txt\n","ALIGN_cats-vs-dogs-large.pkl\t\t\t text_dog_embeddings.pkl\n","ALIGN_imagenet-subset.pkl\t\t\t text_embeddings_correct_align_cat.pkl\n","ALIGN_traffic-signs.pkl\t\t\t\t text_embeddings_correct_align_dog.pkl\n","cats-dogs-big_ids.pkl\t\t\t\t text_embeddings_correct_clip_dog.pkl\n","cats-dogs-big.pkl\t\t\t\t text_embeddings_random_align_cat.pkl\n","cats-vs-dogs-large\t\t\t\t text_embeddings_random_align_dog.pkl\n","CLIP_cats-vs-dogs-large.pkl\t\t\t text_embeddings_random_clip_dog.pkl\n","CLIP_imagenet-subset.pkl\t\t\t text_random_embeddings.pkl\n","CLIP_traffic-signs.pkl\t\t\t\t traffic-signs\n","dog_wrong_2_12.txt\t\t\t\t train-small\n","dog_wrong.txt\t\t\t\t\t ViT-CLS_cats-vs-dogs-large.pkl\n","image_embeddings__cats-vs-dogs.pkl\t\t ViT-CLS_imagenet-subset.pkl\n","image_embeddings__traffic-signs.pkl\t\t ViT-CLS_traffic-signs.pkl\n","imagenet_one-of-each-class-except-cats-and-dogs  ViT-pooling_cats-vs-dogs-large.pkl\n","indizes_clean_Anni.txt\t\t\t\t ViT-pooling_imagenet-subset.pkl\n","jellyfish-classification\t\t\t ViT-pooling_traffic-signs.pkl\n"]}]},{"cell_type":"markdown","source":["# Foundation models load"],"metadata":{"id":"VExpsXZhVlwz"}},{"cell_type":"code","source":["def get_embeddings_batchwise(all_labels, model, preprocess, batch_size=64):\n","  currently_processed =[]\n","  embeddings = []\n","  for n_instances_processed, label in enumerate(all_labels):\n","\n","    currently_processed.append(label)\n","\n","    # Get embeddings if already a batch is full\n","    if n_instances_processed % batch_size == 0 and n_instances_processed > 0 or n_instances_processed == len(all_labels) - 1:\n","      for i in currently_processed:\n","        with torch.no_grad():\n","          embeddings.append(model.encode_text(tokenize(i).to(device)))\n","        #embeddings.append(model.encode_text(clip.tokenize(currently_processed).to(device)))\n","      currently_processed =[]\n","      print(n_instances_processed, 'loaded and encoded')\n","\n","  return torch.cat(embeddings)"],"metadata":{"id":"14-It_OIWJeI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load clip or align"],"metadata":{"id":"F7ZrMWSUWPIf"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","\n","class Align(torch.nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.align = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    return self.align(x)\n","\n","  def encode_image(self, img: torch.Tensor) -> torch.Tensor:\n","    return self.align.get_image_features(img)\n","\n","  def encode_text(self, text: BatchEncoding) -> torch.Tensor:\n","    return self.align.get_text_features(**text)\n","\n","\n","def align_preprocessor_with_memory_fix(img) -> torch.Tensor:\n","  processor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\n","  with torch.no_grad():\n","    processed = processor(images=img, return_tensors=\"pt\").to(device).pixel_values.squeeze(0)\n","  del processor\n","  return processed\n","\n","\n","if MODEL == 'CLIP':\n","  model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","  tokenize = clip.tokenize\n","elif MODEL == 'ALIGN':\n","  model = Align().to(device)\n","  preprocess = align_preprocessor_with_memory_fix\n","  tokenizer = AutoTokenizer.from_pretrained(\"kakaobrain/align-base\")\n","  tokenize = lambda s: tokenizer([s], padding=True, return_tensors=\"pt\")\n","else:\n","  raise ValueError(f'Invalid model {MODEL} selected!')\n"],"metadata":{"id":"tJ7pp-v7VtGr","executionInfo":{"status":"ok","timestamp":1707247291035,"user_tz":-60,"elapsed":12510,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"43acdb11-e408-42bb-f44a-4f2cc7326612"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 100MiB/s]\n"]}]},{"cell_type":"markdown","source":["# DOG case\n","Generate and store only the embeddings for dataset creation for ML approach\n","- randomized text embeddings for correct and incorrect images"],"metadata":{"id":"1luWablvWWyv"}},{"cell_type":"code","source":["additional_classes = [ 'car', 'bicycle', 'flower', 'sunset', 'beach', 'mountain', 'skyline', 'food', 'people',\n","                      'portrait', 'wildlife', 'landscape', 'cityscape', 'architecture', 'street', 'travel', 'vacation',\n","                       'sports', 'basketball', 'soccer', 'tennis', 'gymnastics', 'swimming', 'yoga', 'music', 'concert',\n","                       'festival', 'guitar', 'piano', 'art', 'painting', 'sculpture', 'abstract', 'fashion', 'clothing',\n","                       'jewelry', 'makeup', 'hair', 'wedding', 'technology', 'smartphone', 'laptop', 'camera', 'drone', 'nature',\n","                       'forest', 'river', 'desert', 'wildlife', 'bird', 'insect', 'fish', 'reptile', 'fruit', 'vegetable',\n","                       'coffee', 'tea', 'wine', 'beer', 'cocktail', 'vintage', 'retro', 'urban', 'rural', 'portrait', 'selfie',\n","                       'family', 'friends', 'love', 'romance', 'wedding', 'baby', 'childhood', 'home', 'garden', 'interior',\n","                       'exterior', 'sunset', 'sunrise', 'moon', 'stars', 'space', 'astronomy', 'weather', 'rain',\n","                      'snow', 'fireworks', 'festive', 'celebration', 'holiday', 'halloween', 'christmas', 'easter',\n","                       'surprise', 'temple', 'sadness', 'hapiness', 'game', 'cat', 'person', 'plant', 'furniture', 'memory']"],"metadata":{"id":"hdLwX7gLWbL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import string\n","text_random = [''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(1000)]+[f\"a picture of {obj}\" for obj in additional_classes]\n","\n","text_dog = [\"a dog\", \"a picture of dog\", \"a picture of people with dog\", \"a picture of dog playing\",\n","                            \"a picture of dog sleeping\", \"dog picture\",\"dog drawing\", \"a drawing of dog\",\n","                            \"a picture of many dogs\",\"a picture of dog with objects\",\n","             \"a picture of dog in nature\"] + ['a picture of dog '+ ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(100)]+ [\n","                 'a picture of people with dog '+ ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(100)] + [\n","                 'a picture of dog with objects '+ ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(100)] + [\n","                 'a picture of many dogs '+ ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(100)] + [\n","                 'a picture of dog with '+ ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(100)]\n","\n"],"metadata":{"id":"WpnzMjfpWcZT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_random_embeddings = get_embeddings_batchwise(text_random, model, preprocess)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMnPaeglWhLg","executionInfo":{"status":"ok","timestamp":1706787391878,"user_tz":-60,"elapsed":104547,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"56a949aa-83d6-485e-9b12-c652341dbe72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["64 loaded and encoded\n","128 loaded and encoded\n","192 loaded and encoded\n","256 loaded and encoded\n","320 loaded and encoded\n","384 loaded and encoded\n","448 loaded and encoded\n","512 loaded and encoded\n","576 loaded and encoded\n","640 loaded and encoded\n","704 loaded and encoded\n","768 loaded and encoded\n","832 loaded and encoded\n","896 loaded and encoded\n","960 loaded and encoded\n","1024 loaded and encoded\n","1088 loaded and encoded\n","1102 loaded and encoded\n"]}]},{"cell_type":"code","source":["text_dog_embeddings = get_embeddings_batchwise(text_dog, model, preprocess)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V7itBi9mWkEl","executionInfo":{"status":"ok","timestamp":1706787446844,"user_tz":-60,"elapsed":54980,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"b2f238c2-8048-4c2f-b4df-d80650db1b10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["64 loaded and encoded\n","128 loaded and encoded\n","192 loaded and encoded\n","256 loaded and encoded\n","320 loaded and encoded\n","384 loaded and encoded\n","448 loaded and encoded\n","510 loaded and encoded\n"]}]},{"cell_type":"code","source":["pickle_file_rand = datasets_path + f\"{RANDOM_TEXT_EMBEDDINGS_NAME_BASE}_dog.pkl\"\n","pickle_file_dog = datasets_path + f\"{CORRECT_TEXT_EMBEDDINGS_NAME_BASE}_dog.pkl\"\n","\n","with open(pickle_file_rand, 'wb') as f:\n","  pickle.dump(text_random_embeddings, f)\n","\n","with open(pickle_file_dog, 'wb') as f:\n","  pickle.dump(text_dog_embeddings, f)"],"metadata":{"id":"0X1IeWklWnTN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CAT case"],"metadata":{"id":"yGpkPnme_ob8"}},{"cell_type":"code","source":["additional_classes = [ 'car', 'bicycle', 'flower', 'sunset', 'beach', 'mountain', 'skyline', 'food', 'people',\n","                      'portrait', 'wildlife', 'landscape', 'cityscape', 'architecture', 'street', 'travel', 'vacation',\n","                       'sports', 'basketball', 'soccer', 'tennis', 'gymnastics', 'swimming', 'yoga', 'music', 'concert',\n","                       'festival', 'guitar', 'piano', 'art', 'painting', 'sculpture', 'abstract', 'fashion', 'clothing',\n","                       'jewelry', 'makeup', 'hair', 'wedding', 'technology', 'smartphone', 'laptop', 'camera', 'drone', 'nature',\n","                       'forest', 'river', 'desert', 'wildlife', 'bird', 'insect', 'fish', 'reptile', 'fruit', 'vegetable',\n","                       'coffee', 'tea', 'wine', 'beer', 'cocktail', 'vintage', 'retro', 'urban', 'rural', 'portrait', 'selfie',\n","                       'family', 'friends', 'love', 'romance', 'wedding', 'baby', 'childhood', 'home', 'garden', 'interior',\n","                       'exterior', 'sunset', 'sunrise', 'moon', 'stars', 'space', 'astronomy', 'weather', 'rain',\n","                      'snow', 'fireworks', 'festive', 'celebration', 'holiday', 'halloween', 'christmas', 'easter',\n","                       'surprise', 'temple', 'sadness', 'hapiness', 'game', 'dog', 'person', 'plant', 'furniture', 'memory']"],"metadata":{"id":"iOLyhCRA_q5g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import string\n","text_random = [''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(1000)]+[f\"a picture of {obj}\" for obj in additional_classes]\n","\n","text_cat = [\"a cat\", \"a picture of cat\", \"a picture of people with cat\", \"a picture of cat playing\",\n","                            \"a picture of cat sleeping\", \"cat picture\",\"cat drawing\", \"a drawing of cat\",\n","                            \"a picture of many cats\",\"a picture of cat with objects\",\n","             \"a picture of cat in nature\"] + ['a picture of cat '+ ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(100)]+ [\n","                 'a picture of people with cat '+ ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(100)] + [\n","                 'a picture of cat with objects '+ ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(100)] + [\n","                 'a picture of many cat '+ ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(100)] + [\n","                 'a picture of cat with '+ ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(random.randint(2,10))) for j in range(100)]\n","\n"],"metadata":{"id":"29kzYjxZ_wea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_random_embeddings = get_embeddings_batchwise(text_random, model, preprocess)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pzx2CNpB_yIc","executionInfo":{"status":"ok","timestamp":1707247471226,"user_tz":-60,"elapsed":180203,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"2440eb9b-eacc-4fe5-c5eb-5fce4bad3363"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["64 loaded and encoded\n","128 loaded and encoded\n","192 loaded and encoded\n","256 loaded and encoded\n","320 loaded and encoded\n","384 loaded and encoded\n","448 loaded and encoded\n","512 loaded and encoded\n","576 loaded and encoded\n","640 loaded and encoded\n","704 loaded and encoded\n","768 loaded and encoded\n","832 loaded and encoded\n","896 loaded and encoded\n","960 loaded and encoded\n","1024 loaded and encoded\n","1088 loaded and encoded\n","1102 loaded and encoded\n"]}]},{"cell_type":"code","source":["text_cat_embeddings = get_embeddings_batchwise(text_cat, model, preprocess)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gNNUyJ_Z_z1r","executionInfo":{"status":"ok","timestamp":1707247553370,"user_tz":-60,"elapsed":82156,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"6f3ebea4-61d4-40b9-b7ae-2800d49f1b67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["64 loaded and encoded\n","128 loaded and encoded\n","192 loaded and encoded\n","256 loaded and encoded\n","320 loaded and encoded\n","384 loaded and encoded\n","448 loaded and encoded\n","510 loaded and encoded\n"]}]},{"cell_type":"code","source":["pickle_file_rand = datasets_path + f\"{RANDOM_TEXT_EMBEDDINGS_NAME_BASE}_cat.pkl\"\n","pickle_file_cat = datasets_path + f\"{CORRECT_TEXT_EMBEDDINGS_NAME_BASE}_cat.pkl\"\n","\n","\n","with open(pickle_file_rand, 'wb') as f:\n","  pickle.dump(text_random_embeddings, f)\n","\n","with open(pickle_file_cat, 'wb') as f:\n","  pickle.dump(text_cat_embeddings, f)"],"metadata":{"id":"n3kFfiUq_1i9"},"execution_count":null,"outputs":[]}]}