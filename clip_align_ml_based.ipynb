{"cells":[{"cell_type":"markdown","metadata":{"id":"M5zokF87VQGp"},"source":["# Choose settings"]},{"cell_type":"markdown","metadata":{"id":"Lu2CK4YAl9qA"},"source":["##### Choose your settings here"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"y4Ef3a0WVPdF","executionInfo":{"status":"ok","timestamp":1707296328953,"user_tz":-60,"elapsed":314,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"outputs":[],"source":["DOG_OR_CAT = \"cat\" #can also be \"dog\"\n","RUN = 2 #update if don't want overwritten results\n","TYPE_EXTERNAL_POLLUTION = \"cross-label\" #other option: \"imagenet_only\". With cross-label: 1% pollution, 1/2 of that cross-label, other 1/2 imagenet\n","MODEL = \"CLIP\"\n","\n","\n","# choose dataset\n","DATASET_NAME = 'cats-vs-dogs-large'  # needs to match folder name in FM/datasets\n","LOAD_AND_EMBED_DATASET_IN_BATCHES = True  # True for large datasets, False for small ones\n","USE_CACHED_EMBEDDINGS = f'{MODEL}_cats-vs-dogs-large.pkl'  # '' for loading the dataset normally, 'CREATE__{x}.pkl' for creating the cache file {x}.pkl, '{x}.pkl' for loading the cache file {x}.pkl\n","MISLABELED_INSTANCES = 'mislabeled_instances_cats-vs-dogs.pkl'  # if not '', but e.g. 'mislabeled_instances_cats-vs-dogs.pkl', the pickle file specifies which files to drop from the loaded embeddings\n","\n","# choose how many image-label mismatches to insert\n","MISMATCH_PORTION = 0.01  # percentage of mismatching image-label pairs added\n","MANIPULATION_TYPES = [0.5, 0.5, 0.0, 0.0]  # how much of the MISMATCH_PORTION to produce by [exchanging images between classes, inserting images from other datasets, inserting randomly generated images, inserting placeholder images]\n","IMAGENET_EMBEDDINGS = f'{MODEL}_imagenet-subset.pkl'  # specify if MANIPULATION_TYPES[1] > 0\n","\n","CORRECT_TEXT_EMBEDDINGS_NAME_BASE = f\"text_embeddings_correct_{MODEL.lower()}_{DOG_OR_CAT}\"\n","RANDOM_TEXT_EMBEDDINGS_NAME_BASE = f\"text_embeddings_random_{MODEL.lower()}_{DOG_OR_CAT}\"\n","\n","RESULTS_FILE = f\"results_{MODEL.lower()}_{DOG_OR_CAT}_run_{RUN}\""]},{"cell_type":"markdown","metadata":{"id":"Voe1lmjsl8_c"},"source":["##### This part is calculated automatically"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VRv_RrFZl5pb","executionInfo":{"status":"ok","timestamp":1707295505833,"user_tz":-60,"elapsed":279,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"outputs":[],"source":["datasets_path = '/content/drive/My Drive/FM/datasets/'\n","dataset_path = datasets_path + DATASET_NAME + '/'\n","\n","if DATASET_NAME == 'cats-vs-dogs-large' or DATASET_NAME == 'train-small':\n","  LABELS = ['cat', 'dog']\n","elif DATASET_NAME == 'jellyfish-classification':\n","  LABELS = ['barrel jellyfish', 'compass jellyfish', 'lions mane jellyfish', 'moon jellyfish']\n","elif DATASET_NAME == 'traffic-signs':\n","  LABELS = ['30 kilometers per hour speed limit traffic sign', '80 kilometers per hour speed limit traffic sign', '100 kilometers per hour speed limit traffic sign', 'give way traffic sign', 'no entry traffic sign', 'no overtaking traffic sign', 'priority over oncoming traffic sign', 'stop sign']\n","else:\n","  raise ValueError('Invalid dataset selected or labels not set!')\n","\n","assert 0 <= MISMATCH_PORTION <= 1, f'MISMATCH_PORTION must be in [0, 1] but is {MISMATCH_PORTION}'\n","assert len(MANIPULATION_TYPES) == 4 and sum(MANIPULATION_TYPES) == 1, f'MANIPULATION_TYPES must contain 4 entries that sum up to 1.0 but is {MANIPULATION_TYPES}'\n","assert MANIPULATION_TYPES[0] + MANIPULATION_TYPES[1] == 1, 'At the moment, only interclass corruption and imagenet corruption are implemented!'"]},{"cell_type":"markdown","metadata":{"id":"-8RYGhGcVQp7"},"source":["# Load libraries"]},{"cell_type":"code","source":["# hacky way when hitting \"run all\" that libraries are not reloaded\n","try:\n","  torch.tensor([[0]])\n","  libraries_already_loaded = True\n","except:\n","  libraries_already_loaded = False"],"metadata":{"id":"ouMUhtN7GcGZ","executionInfo":{"status":"ok","timestamp":1707295509010,"user_tz":-60,"elapsed":295,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"nff6q1HOlfh0","executionInfo":{"status":"ok","timestamp":1707295547764,"user_tz":-60,"elapsed":38461,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2d944363-124c-4b50-877c-5e511cbcd513"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.3\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-59ltn__f\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-59ltn__f\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=1669c4f020ccbf7645ee05eba55cd06ec09106dc1a382da4084109b0e6bbbf08\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-b595wffq/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]}],"source":["if not libraries_already_loaded:\n","  ! pip install ftfy regex tqdm\n","  ! pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22071,"status":"ok","timestamp":1707295569829,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"},"user_tz":-60},"id":"vPsiwNE2nmAB","outputId":"45caecc6-90f3-4871-9bb3-60c656579ea4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Torch version: 2.1.0+cu121\n"]}],"source":["import torch\n","from torchvision import transforms\n","import clip\n","from transformers import AutoImageProcessor, ViTModel, AlignProcessor, AlignModel, AutoTokenizer\n","from transformers.tokenization_utils_base import BatchEncoding\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","from pkg_resources import packaging\n","import os\n","from google.colab import drive\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n","import glob\n","import pickle\n","from scipy.spatial.distance import cosine\n","\n","print(\"Torch version:\", torch.__version__)\n","\n","#from sklearn.metrics.pairwise import cosine_similarity #as cosine_similarity_sklearn\n","#def cosine_similarity(X, Y=None):\n","#  X = X.clone().detach().numpy()\n","#  if Y is not None:\n","#    Y = Y.clone().detach().numpy()\n","#  return torch.tensor(cosine_similarity_sklearn(X, Y))"]},{"cell_type":"code","source":["def calculate_column_average(matrix):\n","    \"\"\"\n","    Calculates the average for each column in a PyTorch matrix.\n","\n","    Parameters:\n","    - matrix: torch.Tensor\n","        The input matrix for which the column averages are to be calculated.\n","\n","    Returns:\n","    - torch.Tensor\n","        A tensor containing the average value for each column of the input matrix.\n","\n","    Raises:\n","    - TypeError:\n","        Raises an error if the input is not a PyTorch tensor.\n","    - ValueError:\n","        Raises an error if the input tensor is empty or has zero columns.\n","    \"\"\"\n","\n","    # Checking if the input is a PyTorch tensor\n","    if not isinstance(matrix, torch.Tensor):\n","        raise TypeError(\"Input should be a PyTorch tensor.\")\n","\n","    # Checking if the input tensor is empty or has zero columns\n","    if matrix.numel() == 0 or matrix.size(1) == 0:\n","        raise ValueError(\"Input tensor is empty or has zero columns.\")\n","\n","    # Calculating the column averages but taking out the entry for the same vector in the matrix (diagonal entry)\n","    column_sums = torch.sum(matrix, dim=0)\n","    column_counts = torch.tensor([matrix.size(0)] * (matrix.size(1)), dtype=torch.float32)\n","    column_averages = (column_sums -1) / (column_counts - 1)\n","\n","    return column_averages\n","\n","def cosine_similarity_matrix(embeddings_tensor):\n","    \"\"\"\n","    Function to compute a similarity matrix using dot product\n","\n","    Parameters:\n","    - embeddings: list of torch.Tensor\n","        List of embeddings of images produced with CLIP.\n","\n","    Returns:\n","    - similarity_matrix: numpy.ndarray\n","        2D numpy array representing the similarity matrix between the embeddings.\n","        Each element (i, j) in the matrix represents the similarity between embeddings[i] and embeddings[j].\n","        The similarity score between the two embeddings is calculated using cos similarity. The score ranges from 0 to 1,\n","        where 0 indicates completely dissimilar embeddings and 1 indicates identical embeddings.\n","    \"\"\"\n","\n","    \"\"\"\n","    # Calculating the cosine distance between the two embeddings\n","    distance = cosine(embedding1, embedding2)\n","\n","    # Converting the distance to similarity score\n","    similarity = 1 - distance\n","\n","    return similarity\n","    \"\"\"\n","\n","    # Normalizing the embeddings\n","    embeddings_tensor = torch.nn.functional.normalize(embeddings_tensor, dim=1)\n","\n","    # Computing the similarity matrix using dot product\n","    similarity_matrix = torch.matmul(embeddings_tensor, embeddings_tensor.T)\n","\n","    return similarity_matrix\n","\n","from sklearn.metrics.pairwise import cosine_similarity as cosine_similarity_sklearn\n","def cosine_similarity(x, y=None):\n","  if y is None:\n","    return cosine_similarity_matrix(x)\n","  return cosine_similarity_sklearn(x, y)"],"metadata":{"id":"2OoiHPpeNC7S","executionInfo":{"status":"ok","timestamp":1707295569829,"user_tz":-60,"elapsed":10,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lkIgVhzGhreu"},"source":["# Mounting storage"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39249,"status":"ok","timestamp":1707295609070,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"},"user_tz":-60},"id":"RIOqbAUohxeV","outputId":"3c8e0eb1-5040-495f-e0d7-22f0728f17da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","AlexNet_cats-vs-dogs-large.pkl\t\t\t mislabeled_instances_cats-vs-dogs.pkl\n","AlexNet_imagenet-subset.pkl\t\t\t note.txt\n","AlexNet_traffic-signs.pkl\t\t\t text_dog_embeddings.pkl\n","ALIGN_cats-vs-dogs-large.pkl\t\t\t text_embeddings_correct_align_cat.pkl\n","ALIGN_imagenet-subset.pkl\t\t\t text_embeddings_correct_align_dog.pkl\n","ALIGN_traffic-signs.pkl\t\t\t\t text_embeddings_correct_clip_cat.pkl\n","cats-dogs-big_ids.pkl\t\t\t\t text_embeddings_correct_clip_dog.pkl\n","cats-dogs-big.pkl\t\t\t\t text_embeddings_random_align_cat.pkl\n","cats-vs-dogs-large\t\t\t\t text_embeddings_random_align_dog.pkl\n","CLIP_cats-vs-dogs-large.pkl\t\t\t text_embeddings_random_clip_cat.pkl\n","CLIP_imagenet-subset.pkl\t\t\t text_embeddings_random_clip_dog.pkl\n","CLIP_traffic-signs.pkl\t\t\t\t text_random_embeddings.pkl\n","dog_wrong_2_12.txt\t\t\t\t traffic-signs\n","dog_wrong.txt\t\t\t\t\t train-small\n","image_embeddings__cats-vs-dogs.pkl\t\t ViT-CLS_cats-vs-dogs-large.pkl\n","image_embeddings__traffic-signs.pkl\t\t ViT-CLS_imagenet-subset.pkl\n","imagenet_one-of-each-class-except-cats-and-dogs  ViT-CLS_traffic-signs.pkl\n","indizes_clean_Anni.txt\t\t\t\t ViT-pooling_cats-vs-dogs-large.pkl\n","jellyfish-classification\t\t\t ViT-pooling_imagenet-subset.pkl\n","mislabeled_instances_cats-vs-dogs_CATS_ONLY.pkl  ViT-pooling_traffic-signs.pkl\n"]}],"source":["drive.mount('/content/drive')\n","!ls \"{datasets_path}\""]},{"cell_type":"markdown","metadata":{"id":"aUki7WXfjgwC"},"source":["# Define dataset loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAu1VUDTjw1t"},"outputs":[],"source":["def load_dataset(folder_path, labels):\n","\n","    # Checking if the provided folder path exists\n","    if not os.path.exists(folder_path):\n","        raise ValueError(\"Folder path does not exist.\")\n","\n","    images = {}\n","    for label in labels:\n","      images[label] = []\n","\n","    # Looping through all files in the folder\n","    for i, filename in enumerate(glob.glob(folder_path + '**/*', recursive=True)):\n","\n","      if i % 1000 == 0:\n","        print(i, 'files loaded')\n","\n","      try:\n","        img = Image.open(filename).convert('RGB')\n","      except:\n","        continue\n","\n","      label_found = False\n","      for label in labels:\n","        if label in '/'.join(filename.split('/')[-2:]):\n","          if label_found:\n","            raise ValueError(f\"Label of {filename} is ambiguous.\")\n","          label_found = True\n","          images[label].append(img)\n","\n","      if not label_found:\n","        raise ValueError(f\"No label for {filename} found.\")\n","\n","    print(i+1, 'files loaded')\n","\n","    return images\n","\n","def get_embeddings_dict_batchwise(folder_path, labels, model, preprocess, batch_size=64):\n","\n","  # Checking if the provided folder path exists\n","  if not os.path.exists(folder_path):\n","    raise ValueError(\"Folder path does not exist.\")\n","\n","  image_embeddings = {}\n","  for label in labels:\n","    image_embeddings[label] = []\n","\n","  images = {}\n","  for label in labels:\n","    images[label] = []\n","\n","  # Looping through all files in the folder\n","  all_files = glob.glob(folder_path + '**/*', recursive=True)\n","  for n_instances_processed, filename in enumerate(all_files):\n","\n","    try:\n","      img = Image.open(filename).convert('RGB')\n","    except:\n","      continue\n","\n","    # Find label of the image\n","    label_found = False\n","    for label in labels:\n","      if label in '/'.join(filename.split('/')[-2:]):\n","        if label_found:\n","          raise ValueError(f\"Label of {filename} is ambiguous.\")\n","        label_found = True\n","        images[label].append(img)\n","    if not label_found:\n","      raise ValueError(f\"No label for {filename} found.\")\n","\n","    # Get embeddings if already a batch is full\n","    if n_instances_processed % batch_size == 0 and n_instances_processed > 0 or n_instances_processed == len(all_files) - 1:\n","      for label in labels:\n","        if len(images[label]) == 0:\n","          continue\n","        with torch.no_grad():\n","          processed_images = torch.cat(([preprocess(img).unsqueeze(0) for img in images[label]]))\n","          image_embeddings[label].append(model.encode_image(processed_images.to(device)))\n","          del processed_images\n","      images = {}\n","      for label in labels:\n","        images[label] = []\n","      print(n_instances_processed, 'loaded and encoded')\n","\n","  # Convert list of embeddings to tensor\n","  for label in labels:\n","    image_embeddings[label] = torch.cat((image_embeddings[label]))\n","\n","  return image_embeddings"]},{"cell_type":"markdown","metadata":{"id":"SMoCzhM7Cpva"},"source":["# Load and embed dataset"]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","\n","class Align(torch.nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.align = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    return self.align(x)\n","\n","  def encode_image(self, img: torch.Tensor) -> torch.Tensor:\n","    return self.align.get_image_features(img)\n","\n","  def encode_text(self, text: BatchEncoding) -> torch.Tensor:\n","    return self.align.get_text_features(**text)\n","\n","\n","def align_preprocessor_with_memory_fix(img) -> torch.Tensor:\n","  processor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\n","  with torch.no_grad():\n","    processed = processor(images=img, return_tensors=\"pt\").to(device).pixel_values.squeeze(0)\n","  del processor\n","  return processed\n","\n","\n","if MODEL == 'CLIP':\n","  model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","  tokenize = clip.tokenize\n","elif MODEL == 'ALIGN':\n","  model = Align().to(device)\n","  preprocess = align_preprocessor_with_memory_fix\n","  tokenizer = AutoTokenizer.from_pretrained(\"kakaobrain/align-base\")\n","  tokenize = lambda s: tokenizer([s], padding=True, return_tensors=\"pt\")\n","else:\n","  raise ValueError(f'Invalid model {MODEL} selected!')"],"metadata":{"id":"YzVKInFRnypu","executionInfo":{"status":"ok","timestamp":1707246091301,"user_tz":-60,"elapsed":14704,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a0faf778-af53-4f78-84c4-baa0432fcd62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 129MiB/s]\n"]}]},{"cell_type":"code","source":["if USE_CACHED_EMBEDDINGS != '' and USE_CACHED_EMBEDDINGS.split('__')[0] != 'CREATE':\n","\n","  # load embeddings of previous execution from pickle file\n","  pickle_file = datasets_path + USE_CACHED_EMBEDDINGS\n","  with open(pickle_file, 'rb') as f:\n","    image_embeddings = pickle.load(f)\n","\n","  print('Embeddings loaded from', pickle_file)\n","\n","else:\n","\n","  if LOAD_AND_EMBED_DATASET_IN_BATCHES:\n","\n","    # load and embed images in batches (to save GPU memory and especially RAM)\n","    image_embeddings = get_embeddings_dict_batchwise(dataset_path, LABELS, model, preprocess, batch_size=512)\n","\n","  else:\n","\n","    # load images\n","    images = load_dataset(dataset_path, LABELS)\n","\n","    # embed images and text\n","    image_embeddings = {}\n","    for label in LABELS:\n","      processed_images = torch.cat(([preprocess(img).unsqueeze(0) for img in images[label]])).to(device)\n","      with torch.no_grad():\n","        image_embeddings[label] = model.encode_image(processed_images)\n","\n","  # move embeddings to cpu and convert to suitable datatype for further analysis\n","  for key in image_embeddings:\n","    image_embeddings[key] = image_embeddings[key].cpu().type(torch.float)\n","\n","  # save embeddings in pickle file if desired (enabled to reload them later on)\n","  if USE_CACHED_EMBEDDINGS != '' and USE_CACHED_EMBEDDINGS.split('__')[0] == 'CREATE':\n","    pickle_filename = '__'.join(USE_CACHED_EMBEDDINGS.split('__')[1:])  # remove prefix 'CREATE__'\n","    pickle_file = datasets_path + pickle_filename\n","    with open(pickle_file, 'wb') as f:\n","      pickle.dump(image_embeddings, f)\n","    print('Embeddings stored in', pickle_file)\n","\n","# save embedding dimension for creation of reference vectors etc.\n","embedding_dim = image_embeddings[LABELS[0]].shape[1]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OM6pCR1ibfGv","executionInfo":{"status":"ok","timestamp":1707247645115,"user_tz":-60,"elapsed":660,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"a4712588-38aa-4943-e61b-ea4e65511879"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings loaded from /content/drive/My Drive/FM/datasets/CLIP_cats-vs-dogs-large.pkl\n"]}]},{"cell_type":"code","source":["LABELS_POLLUTION_ALSO = LABELS.copy()\n","mislabeled_indices = None\n","\n","if MISLABELED_INSTANCES != '':\n","  with open(datasets_path + MISLABELED_INSTANCES, 'rb') as f:\n","    mislabeled_indices = pickle.load(f)\n","  for label in mislabeled_indices:\n","    if len(mislabeled_indices[label]) == 0 or sum(mislabeled_indices[label]) == 0:\n","      continue\n","    image_embeddings[label + '_pollution'] = image_embeddings[label][mislabeled_indices[label]].clone().detach()\n","    image_embeddings[label + '_clean'] = image_embeddings[label][[not i for i in mislabeled_indices[label]]].clone().detach()\n","    LABELS_POLLUTION_ALSO += [label + '_pollution', label + '_clean']\n","\n","if MANIPULATION_TYPES[1] > 0:\n","  pickle_file = datasets_path + IMAGENET_EMBEDDINGS\n","  with open(pickle_file, 'rb') as f:\n","    image_embeddings['imagenet_subset'] = pickle.load(f)['val']\n","  print('Embeddings loaded from', pickle_file)\n","  LABELS_POLLUTION_ALSO += ['imagenet_subset']\n","\n","print('LABELS =', LABELS)\n","print('LABELS_POLLUTION_ALSO =', LABELS_POLLUTION_ALSO)\n","for k in image_embeddings:\n","  print(k, len(image_embeddings[k]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfSc3hCv1A3E","executionInfo":{"status":"ok","timestamp":1707247645628,"user_tz":-60,"elapsed":12,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"408a2365-2adb-433a-c03e-f38930a0bcb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings loaded from /content/drive/My Drive/FM/datasets/CLIP_imagenet-subset.pkl\n","LABELS = ['cat', 'dog']\n","LABELS_POLLUTION_ALSO = ['cat', 'dog', 'cat_pollution', 'cat_clean', 'dog_pollution', 'dog_clean', 'imagenet_subset']\n","cat 12502\n","dog 12499\n","cat_pollution 25\n","cat_clean 12477\n","dog_pollution 24\n","dog_clean 12475\n","imagenet_subset 869\n"]}]},{"cell_type":"code","source":["MISLABELED_INSTANCES_LIST = 'mislabeled_instances_cats-vs-dogs.pkl'\n","\n","with open(datasets_path + MISLABELED_INSTANCES_LIST, 'rb') as f:\n","  mislabeled_indices = pickle.load(f)"],"metadata":{"id":"LoUhtMXQkSRm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Imagenet pics in array for easier handling with dataset creation (test phase external pollution)"],"metadata":{"id":"eRJlWFYzQtHJ"}},{"cell_type":"code","source":["imagenet_arr = [ image_embeddings[\"imagenet_subset\"][i,:].numpy() for i in range(image_embeddings[\"imagenet_subset\"].shape[0])]"],"metadata":{"id":"uoKoQnJgQsnJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DL Dataset preparation"],"metadata":{"id":"j5A0exVHcS8B"}},{"cell_type":"markdown","source":["## General preparations\n"],"metadata":{"id":"d2prI07gcZMI"}},{"cell_type":"markdown","source":["### Load textual embeddings"],"metadata":{"id":"HAQq6C8j03hr"}},{"cell_type":"code","source":["def load_from_pickle (datasets_path, USE_CACHED_EMBEDDINGS =\"cats-vs-dogs.pkl\" ):\n","  image_embeddings = []\n","  pickle_file = datasets_path + USE_CACHED_EMBEDDINGS\n","  with open(pickle_file, 'rb') as f:\n","    image_embeddings = pickle.load(f)\n","  return image_embeddings"],"metadata":{"id":"bbt8-p32dZ4Z","executionInfo":{"status":"ok","timestamp":1707295692746,"user_tz":-60,"elapsed":251,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["text_random_embeddings = load_from_pickle (datasets_path, USE_CACHED_EMBEDDINGS =f\"{RANDOM_TEXT_EMBEDDINGS_NAME_BASE}.pkl\" )\n","text_dog_embeddings = load_from_pickle (datasets_path, USE_CACHED_EMBEDDINGS =f\"{CORRECT_TEXT_EMBEDDINGS_NAME_BASE}.pkl\" )"],"metadata":{"id":"8dVY-keJ09yN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(text_random_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHPVrVEXmmFB","executionInfo":{"status":"ok","timestamp":1707247647330,"user_tz":-60,"elapsed":7,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"c4aef111-00d9-4918-e6e2-e4b04f037cea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1103"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["len(text_dog_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMLiF8bQmoSa","executionInfo":{"status":"ok","timestamp":1707247647330,"user_tz":-60,"elapsed":6,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"87ae0f1d-f85f-46f6-a6bd-8a42aaaab1c8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["511"]},"metadata":{},"execution_count":73}]},{"cell_type":"markdown","source":["### Fcts for similarity calculation"],"metadata":{"id":"vUZ7W2lg0s7d"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from scipy.spatial.distance import cosine\n","\n","def calc_similarity_matrix_embeddings(embeddings_tensor, embeddings_tensor2 = True):\n","    \"\"\"\n","    Function to compute a similarity matrix using dot product\n","\n","    Parameters:\n","    - embeddings: list of torch.Tensor\n","        List of embeddings of images produced with CLIP.\n","\n","    Returns:\n","    - similarity_matrix: numpy.ndarray\n","        2D numpy array representing the similarity matrix between the embeddings.\n","        Each element (i, j) in the matrix represents the similarity between embeddings[i] and embeddings[j].\n","        The similarity score between the two embeddings is calculated using cos similarity. The score ranges from 0 to 1,\n","        where 0 indicates completely dissimilar embeddings and 1 indicates identical embeddings.\n","    \"\"\"\n","\n","    \"\"\"\n","    # Calculating the cosine distance between the two embeddings\n","    distance = cosine(embedding1, embedding2)\n","\n","    # Converting the distance to similarity score\n","    similarity = 1 - distance\n","\n","    return similarity\n","    \"\"\"\n","    if isinstance(embeddings_tensor2, bool):\n","      embeddings_tensor2 = embeddings_tensor\n","\n","    # Normalizing the embeddings\n","    embeddings_tensor = torch.nn.functional.normalize(embeddings_tensor, dim=1)\n","    embeddings_tensor2 = torch.nn.functional.normalize(embeddings_tensor2, dim=1)\n","\n","    # Computing the similarity matrix using dot product\n","    similarity_matrix = torch.matmul(embeddings_tensor, embeddings_tensor2.T)\n","\n","    return similarity_matrix"],"metadata":{"id":"dc9BjdIRglgW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_column_average(matrix):\n","    \"\"\"\n","    Calculates the average for each column in a PyTorch matrix.\n","\n","    Parameters:\n","    - matrix: torch.Tensor\n","        The input matrix for which the column averages are to be calculated.\n","\n","    Returns:\n","    - torch.Tensor\n","        A tensor containing the average value for each column of the input matrix.\n","\n","    Raises:\n","    - TypeError:\n","        Raises an error if the input is not a PyTorch tensor.\n","    - ValueError:\n","        Raises an error if the input tensor is empty or has zero columns.\n","    \"\"\"\n","\n","    # Checking if the input is a PyTorch tensor\n","    if not isinstance(matrix, torch.Tensor):\n","        raise TypeError(\"Input should be a PyTorch tensor.\")\n","\n","    # Checking if the input tensor is empty or has zero columns\n","    if matrix.numel() == 0 or matrix.size(1) == 0:\n","        raise ValueError(\"Input tensor is empty or has zero columns.\")\n","\n","    # Calculating the column averages but taking out the entry for the same vector in the matrix (diagonal entry)\n","    column_sums = torch.sum(matrix, dim=0)\n","    column_counts = torch.tensor([matrix.size(0)] * (matrix.size(1)), dtype=torch.float32)\n","    column_averages = (column_sums -1) / (column_counts - 1)\n","\n","    return column_averages"],"metadata":{"id":"WeEQqfZ_gphC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["similarity_matrix = calc_similarity_matrix_embeddings(image_embeddings[DOG_OR_CAT])\n","avg_similarity_imgs = calculate_column_average(similarity_matrix)"],"metadata":{"id":"TseOFqsREYCB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Approach 1: Training on embeddings"],"metadata":{"id":"IiRai1_nc1SW"}},{"cell_type":"markdown","source":["### Helper fcts"],"metadata":{"id":"fC03u_O5eOp6"}},{"cell_type":"code","source":["import random\n","import math\n","\n","def generate_random_vectors(n: int, size: int) -> list:\n","\n","    vectors = []\n","    for _ in range(n):\n","        vector = [random.uniform(-1, 1) for _ in range(size)]\n","        vectors.append(vector)\n","\n","    return vectors\n","\n","def normalize_vectors(vectors: list) -> list:\n","\n","    normalized_vectors = []\n","    for vector in vectors:\n","        magnitude = math.sqrt(sum([x**2 for x in vector]))\n","        normalized_vector = [x / magnitude for x in vector]\n","        normalized_vectors.append(torch.tensor(normalized_vector))\n","\n","    return normalized_vectors\n"],"metadata":{"id":"KquWKFlmeQnI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_train_assumed_correct(dataset_img_embeddings, text_random_embeddings,\n","                                 text_class_embeddings, avg_similarity_imgs,\n","                                  random_vec_size = 512):\n","\n","\n","\n","\n","  import string\n","  import random\n","\n","  # Assuming X is your data matrix (features) and y is the corresponding labels\n","  # Make sure to replace this with your actual dataset\n","\n","  # Split the data into training and testing sets\n","  dict_self_similarity = {}\n","  for i, el in enumerate(avg_similarity_imgs):\n","      dict_self_similarity[i] = el\n","\n","\n","  sorted_similarities = sorted(dict_self_similarity.items(), key=lambda x: x[1])\n","  n = len(dict_self_similarity)\n","  q3_index = int((3 *n )// 6)\n","  q4_index = int((4 * n) // 6)\n","  assumed_correct = sorted_similarities[q3_index:q4_index]\n","\n","   # Generate random vectors\n","  random_vectors = generate_random_vectors(int(4*n/18)+ text_class_embeddings.shape[0] - text_random_embeddings.shape[0], random_vec_size)\n","\n","  # Normalize the random vectors\n","  normalized_random_vectors = normalize_vectors(random_vectors)\n","  extr_embed = lambda index: dataset_img_embeddings[index]\n","\n","  embeddings_img_assumed_correct = [ extr_embed(index) for index in dict(assumed_correct).keys()]\n","  train = {}\n","  for emb in (embeddings_img_assumed_correct + [text_class_embeddings[i] for i in range(text_class_embeddings.shape[0])]):\n","    train[emb] = 1\n","  for emb in (normalized_random_vectors +  [text_random_embeddings[i] for i in range(text_random_embeddings.shape[0])]):\n","    train[emb] = 0\n","\n","  l = list(train.items())\n","\n","\n","  X_train = [item[0] for item in l]\n","  y_train = [item[1] for item in l]\n","\n","  return X_train, y_train"],"metadata":{"id":"zcTpV0sPeEgV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset prep"],"metadata":{"id":"J7Qo85ehgudT"}},{"cell_type":"markdown","source":["#### Train dataset"],"metadata":{"id":"yduqaUHwNVK2"}},{"cell_type":"code","source":["random_vectors = generate_random_vectors(2, text_random_embeddings[0].shape[0])\n","\n","  # Normalize the random vectors\n","normalized_random_vectors = normalize_vectors(random_vectors)"],"metadata":{"id":"B24cYw456GsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["similarity_matrix_dog = calc_similarity_matrix_embeddings(image_embeddings[DOG_OR_CAT])\n","avg_similarity_dog_imgs = calculate_column_average(similarity_matrix_dog)"],"metadata":{"id":"k8Ev0pp5gK9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_embeddings, y_train_embeddings = create_train_assumed_correct(dataset_img_embeddings = image_embeddings[DOG_OR_CAT],\n","                                                                      text_random_embeddings = text_random_embeddings,\n","                                                                      text_class_embeddings = text_dog_embeddings,\n","                                                                      avg_similarity_imgs = avg_similarity_dog_imgs, random_vec_size = text_random_embeddings[0].shape[0])\n"],"metadata":{"id":"8Na06KdudiWq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test dataset for internal pollution"],"metadata":{"id":"FIglEl44Ncqr"}},{"cell_type":"code","source":["X_test_embeddings_internal = []\n","y_test_embeddings_internal = []\n","for emb, correct in zip(image_embeddings[DOG_OR_CAT],mislabeled_indices[DOG_OR_CAT]):\n","  X_test_embeddings_internal.append(emb)\n","  if correct == False:\n","    y_test_embeddings_internal.append(1)\n","  else:\n","    y_test_embeddings_internal.append(0)"],"metadata":{"id":"m-iH_AB_MfTS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test dataset for external pollution"],"metadata":{"id":"qlPa_73BNi-T"}},{"cell_type":"code","source":["if TYPE_EXTERNAL_POLLUTION == \"cross-label\":\n","  if DOG_OR_CAT ==\"cat\":\n","    other_label = \"dog\"\n","  else:\n","    other_label = \"cat\"\n","\n","  size_clean = len(image_embeddings[f\"{DOG_OR_CAT}_clean\"].tolist())\n","\n","  X_test_embeddings_external = image_embeddings[f\"{DOG_OR_CAT}_clean\"].tolist() + imagenet_arr[:int(size_clean/198)] + image_embeddings[other_label][:int(size_clean/198)].tolist()\n","  y_test_embeddings_external = [1 for i in range(size_clean)] + [0 for i in range(int(size_clean/99))]\n","\n","else:\n","  X_test_embeddings_external = image_embeddings[f\"{DOG_OR_CAT}_clean\"].tolist() + imagenet_arr\n","  y_test_embeddings_external = [1 for i in range(len(image_embeddings[f\"{DOG_OR_CAT}_clean\"]))] + [0 for i in range(len(imagenet_arr))]"],"metadata":{"id":"deRuG0DQM32s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Models internal pollution"],"metadata":{"id":"nR72nR0GFe3j"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"80C99W33q734"}},{"cell_type":"code","source":["from collections import Counter"],"metadata":{"id":"l4U92uzIYi6g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions_internal ={}\n","predictions_external = {}\n","predicted_filenames_internal = {}"],"metadata":{"id":"B7_u3uceYr7X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## On embeddings"],"metadata":{"id":"6g09mP--RDrk"}},{"cell_type":"markdown","source":["### MLP"],"metadata":{"id":"FQsWA_vlRGE0"}},{"cell_type":"code","source":["from sklearn.neural_network import MLPClassifier\n","\n","clf_mlp = MLPClassifier(solver='sgd',  hidden_layer_sizes=(5, 2), random_state=1)\n","\n","clf_mlp.fit([x.numpy() for x in X_train_embeddings], y_train_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QfabUXLaRFfb","executionInfo":{"status":"ok","timestamp":1707248911587,"user_tz":-60,"elapsed":21911,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"1e159610-a3d0-4429-f81d-eea787e1c927"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(hidden_layer_sizes=(5, 2), random_state=1, solver='sgd')"],"text/html":["<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(5, 2), random_state=1, solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(5, 2), random_state=1, solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":120}]},{"cell_type":"code","source":["predictions_internal[\"Embeddings: MLP (ReLu + SGD)\"] = clf_mlp.predict(np.stack(X_test_embeddings_internal))\n","predictions_external[\"Embeddings: MLP (ReLu + SGD)\"] = clf_mlp.predict(np.stack(X_test_embeddings_external))"],"metadata":{"id":"yBcP7gOhYw4f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MLP with threshold"],"metadata":{"id":"nQMRryl9RKHL"}},{"cell_type":"code","source":["from sklearn.neural_network import MLPClassifier\n","\n","clf_mlp = MLPClassifier(solver='sgd',  hidden_layer_sizes=(5, 2), random_state=1)\n","\n","clf_mlp.fit([x.numpy() for x in X_train_embeddings], y_train_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gG-8YbNzROAD","executionInfo":{"status":"ok","timestamp":1707248923693,"user_tz":-60,"elapsed":10000,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"fd140463-12b5-480c-9936-58505d5c446f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(hidden_layer_sizes=(5, 2), random_state=1, solver='sgd')"],"text/html":["<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(5, 2), random_state=1, solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(5, 2), random_state=1, solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":122}]},{"cell_type":"code","source":["detect_outlier = lambda x: 0 if x >0.4 else 1\n","\n","predictions_internal[\"Embeddings: MLP with threshold (ReLu + SGD)\"] = [detect_outlier(i[0]) for i in clf_mlp.predict_proba(np.stack(X_test_embeddings_internal))]\n","predictions_external[\"Embeddings: MLP with threshold (ReLu + SGD)\"] = [detect_outlier(i[0]) for i in clf_mlp.predict_proba(np.stack(X_test_embeddings_external))]"],"metadata":{"id":"ydkp_C9ySzaY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SVC with sigmoid"],"metadata":{"id":"3EzaSVPzRHxQ"}},{"cell_type":"code","source":["from sklearn import svm\n","\n","clf_sigmoid = svm.SVC(kernel='sigmoid', gamma='scale')\n","\n","clf_sigmoid.fit(np.stack(X_train_embeddings), np.stack(y_train_embeddings))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grqHpm6AROdB","executionInfo":{"status":"ok","timestamp":1707248924993,"user_tz":-60,"elapsed":777,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"5f449b66-4aa1-4a9c-adbc-b6cd008bbe56"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC(kernel='sigmoid')"],"text/html":["<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;sigmoid&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;sigmoid&#x27;)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":124}]},{"cell_type":"code","source":["predictions_internal[\"Embeddings: SVC with sigmoid\"] = clf_sigmoid.predict(np.stack(X_test_embeddings_internal))\n","predictions_external[\"Embeddings: SVC with sigmoid)\"] = clf_sigmoid.predict(np.stack(X_test_embeddings_external))\n"],"metadata":{"id":"osbwuwDmTIY6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SVC with RBF"],"metadata":{"id":"YvlHZ30OR6dh"}},{"cell_type":"code","source":["from sklearn import svm\n","\n","clf_sigmoid = svm.SVC(kernel='rbf', gamma='scale')\n","\n","clf_sigmoid.fit(np.stack(X_train_embeddings), np.stack(y_train_embeddings))"],"metadata":{"id":"TR6h8RYHR5yX","executionInfo":{"status":"ok","timestamp":1707248927251,"user_tz":-60,"elapsed":795,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"48ddf227-e472-4b3f-a3d7-077690eac727"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC()"],"text/html":["<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":126}]},{"cell_type":"code","source":["predictions_internal[\"Embeddings: SVC with RBF\"] = clf_sigmoid.predict(np.stack(X_test_embeddings_internal))\n","predictions_external[\"Embeddings: SVC with RBF\"] = clf_sigmoid.predict(np.stack(X_test_embeddings_external))"],"metadata":{"id":"P1EE7-wFTdhf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ensembles"],"metadata":{"id":"aX7LUb46hknr"}},{"cell_type":"code","source":["num_instances= len(image_embeddings[DOG_OR_CAT])"],"metadata":{"id":"DHIa1mByxGzR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ensemble Embeddings MLP"],"metadata":{"id":"7LfPoF11SvOl"}},{"cell_type":"code","source":["from sklearn.neural_network import MLPClassifier\n","\n","y=[]\n","y_external = []\n","detect_outlier = lambda x: 0 if x >0.48 else 1\n","\n","for r in [random.randint(1,10000)  for i in range(30)]:\n","  clf_mlp = MLPClassifier(solver='sgd',  hidden_layer_sizes=(5, 2), random_state=r)\n","\n","  clf_mlp.fit([x.numpy() for x in X_train_embeddings], y_train_embeddings)\n","\n","  y_pred = [detect_outlier(i[0]) for i in clf_mlp.predict_proba(np.stack(X_test_embeddings_internal))]\n","  if np.count_nonzero(y_pred)>0.995*num_instances:\n","    if  y ==[]:\n","      y=y_pred\n","    else:\n","      for i,e in enumerate(clf_mlp.predict_proba(np.stack(X_test_embeddings_internal))):\n","        if detect_outlier(e[0])==0:\n","          y[i]=0\n","\n","\n","  y_pred = [detect_outlier(i[0]) for i in clf_mlp.predict_proba(np.stack(X_test_embeddings_external))]\n","  if np.count_nonzero(y_pred)>0.995*num_instances:\n","    if  y_external ==[]:\n","      y_external=y_pred\n","    else:\n","      for i,e in enumerate(clf_mlp.predict_proba(np.stack(X_test_embeddings_external))):\n","        if detect_outlier(e[0])==0:\n","          y_external[i]=0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LicKrSz7Suaf","executionInfo":{"status":"ok","timestamp":1707249278200,"user_tz":-60,"elapsed":347351,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"053a0fd5-cd75-4804-ac41-cff0ba032c6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["predictions_internal[\"Embeddings Ensemble MLP (ReLu + SGD)\"] = y\n","predictions_external[\"Embeddings Ensemble MLP (ReLu + SGD)\"] = y_external"],"metadata":{"id":"YFNYHoxbTL76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.count_nonzero(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bV9SVbe3V7PN","executionInfo":{"status":"ok","timestamp":1707249278200,"user_tz":-60,"elapsed":7,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"763a0b97-a5fa-4081-ac7f-7d49ae200989"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12415"]},"metadata":{},"execution_count":131}]},{"cell_type":"markdown","source":["### Ensemble Embeddings SVC and MLP"],"metadata":{"id":"D3Qm2773eSkL"}},{"cell_type":"code","source":["from sklearn.neural_network import MLPClassifier\n","\n","y=[]\n","y_external =[]\n","detect_outlier = lambda x: 0 if x >0.48 else 1\n","\n","for r in [random.randint(1,10000)  for i in range(30)]:\n","  clf_mlp = MLPClassifier(solver='sgd',  hidden_layer_sizes=(5, 2), random_state=r)\n","\n","  clf_mlp.fit([x.numpy() for x in X_train_embeddings], y_train_embeddings)\n","\n","  y_pred = [detect_outlier(i[0]) for i in clf_mlp.predict_proba(np.stack(X_test_embeddings_internal))]\n","  if np.count_nonzero(y_pred)>0.99*num_instances:\n","    if  y ==[]:\n","      y=y_pred\n","    else:\n","      for i,e in enumerate(clf_mlp.predict_proba(np.stack(X_test_embeddings_internal))):\n","        if detect_outlier(e[0])==0:\n","          y[i]=0\n","\n","  y_pred = [detect_outlier(i[0]) for i in clf_mlp.predict_proba(np.stack(X_test_embeddings_external))]\n","  if np.count_nonzero(y_pred)>0.99*num_instances:\n","    if  y_external ==[]:\n","      y_external=y_pred\n","    else:\n","      for i,e in enumerate(clf_mlp.predict_proba(np.stack(X_test_embeddings_external))):\n","        if detect_outlier(e[0])==0:\n","          y_external[i]=0\n","\n","\n","from sklearn import svm\n","\n","clf_sigmoid = svm.SVC(kernel='sigmoid', gamma='scale')\n","\n","clf_sigmoid.fit(np.stack(X_train_embeddings), np.stack(y_train_embeddings))\n","\n","y_pred = clf_sigmoid.predict(np.stack(X_test_embeddings_internal))\n","if np.count_nonzero(y_pred)>0.8*num_instances:\n","  for i,e in enumerate(y_pred):\n","    if e==0:\n","      y[i]=0\n","\n","y_pred = clf_sigmoid.predict(np.stack(X_test_embeddings_external))\n","if np.count_nonzero(y_pred)>0.8*num_instances:\n","  for i,e in enumerate(y_pred):\n","    if e==0:\n","      y_external[i]=0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJ0iaJH6eR-A","executionInfo":{"status":"ok","timestamp":1707249596708,"user_tz":-60,"elapsed":318513,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"aa6ba83a-555a-4b39-81a2-443c521f91b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["predictions_internal[\"Embeddings: Ensemble MLP and SVC\"] = y\n","predictions_external[\"Embeddings: Ensemble MLP and SVC\"] = y_external"],"metadata":{"id":"6C0Ug3nfhWKD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"d10SX9zicVRs"}},{"cell_type":"markdown","source":["## Helper fcts"],"metadata":{"id":"9kqo5bDkshsq"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","def calculate_metrics(prediction, ground_truth):\n","\n","    # Checking if the lengths of the prediction and ground truth arrays are equal.\n","    if len(prediction) != len(ground_truth):\n","        raise ValueError(\"Lengths of prediction and ground truth arrays should be equal.\")\n","\n","    tp = 0\n","    tn = 0\n","    fp = 0\n","    fn = 0\n","    for pred, truth in zip(prediction, ground_truth):\n","      if pred == truth:\n","        if pred == 0:\n","          tp += 1\n","        else:\n","          tn +=1\n","      elif pred == 0:\n","        fp +=1\n","      else:\n","        fn +=1\n","\n","    if (tp+fp) >0:\n","      prec = tp / (tp+fp)\n","    else:\n","      prec=0\n","    if (tp+fn)>0:\n","      rec = tp / (tp+fn)\n","    else:\n","      rec=0\n","    if prec!= 0 and rec !=0:\n","      f1 = 2* prec* rec/(prec+rec)\n","    else:\n","      f1 = 0\n","\n","    print(\"-----\")\n","    print(f\"tp: {tp}\")\n","    print(f\"Precision: {prec}\")\n","    print(f\"recall: {rec}\")\n","    print(f\"f1: {f1}\")\n","\n","    return {\"tp\":tp,\"p\":prec,\"r\":rec,\"f1\":f1}"],"metadata":{"id":"P4q85Vpzbo87"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Eval core"],"metadata":{"id":"Vf52pySmslYv"}},{"cell_type":"code","source":["path_store_results = \"/content/drive/My Drive/FM/results/ML_approaches/\""],"metadata":{"id":"xZBSe_gtrD1R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Internal"],"metadata":{"id":"tPOSW1MeeH3J"}},{"cell_type":"code","source":["log = []\n","for key, predictions in predictions_internal.items():\n","  print()\n","  print()\n","  print(key)\n","  metrics = calculate_metrics(predictions, y_test_embeddings_internal)\n","  metrics[\"0_count\"]=num_instances - np.count_nonzero(predictions)\n","  metrics[\"id\"]=key\n","  log.append(metrics)\n","  print(Counter(predictions).keys()) # equals to list(set(words))\n","  print(Counter(predictions).values())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5FgyUQ3d_Ln","executionInfo":{"status":"ok","timestamp":1707249596718,"user_tz":-60,"elapsed":21,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"b3fe8d6e-2b0a-4599-b86e-6b95cbf9d526"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Embeddings: MLP (ReLu + SGD)\n","-----\n","tp: 13\n","Precision: 0.7647058823529411\n","recall: 0.52\n","f1: 0.6190476190476191\n","dict_keys([1, 0])\n","dict_values([12485, 17])\n","\n","\n","Embeddings: MLP with threshold (ReLu + SGD)\n","-----\n","tp: 13\n","Precision: 0.6842105263157895\n","recall: 0.52\n","f1: 0.5909090909090909\n","dict_keys([1, 0])\n","dict_values([12483, 19])\n","\n","\n","Embeddings: SVC with sigmoid\n","-----\n","tp: 17\n","Precision: 0.5483870967741935\n","recall: 0.68\n","f1: 0.6071428571428571\n","dict_keys([1, 0])\n","dict_values([12471, 31])\n","\n","\n","Embeddings: SVC with RBF\n","-----\n","tp: 9\n","Precision: 1.0\n","recall: 0.36\n","f1: 0.5294117647058824\n","dict_keys([1, 0])\n","dict_values([12493, 9])\n","\n","\n","Embeddings Ensemble MLP (ReLu + SGD)\n","-----\n","tp: 19\n","Precision: 0.21839080459770116\n","recall: 0.76\n","f1: 0.3392857142857143\n","dict_keys([1, 0])\n","dict_values([12415, 87])\n","\n","\n","Embeddings: Ensemble MLP and SVC\n","-----\n","tp: 17\n","Precision: 0.20238095238095238\n","recall: 0.68\n","f1: 0.3119266055045872\n","dict_keys([1, 0])\n","dict_values([12418, 84])\n"]}]},{"cell_type":"code","source":["with open(path_store_results + f\"{RESULTS_FILE}_internal.pkl\" , 'wb') as file:\n","  # Writing the dictionary to the pickle file\n","  pickle.dump(log, file)"],"metadata":{"id":"GwsPomdzrXZl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["External"],"metadata":{"id":"cWQygy76eNKp"}},{"cell_type":"code","source":["TYPE_EXTERNAL_POLLUTION"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"weHS4iv_RLKb","executionInfo":{"status":"ok","timestamp":1707249596718,"user_tz":-60,"elapsed":19,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"46990397-202a-4be6-825a-313630ebeafc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cross-label'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":138}]},{"cell_type":"code","source":["log_external = []\n","for key, predictions in predictions_external.items():\n","  print()\n","  print()\n","  print(key)\n","  metrics = calculate_metrics(predictions, y_test_embeddings_external)\n","  metrics[\"0_count\"]=num_instances - np.count_nonzero(predictions)\n","  metrics[\"id\"]=key\n","  log_external.append(metrics)\n","  print(Counter(predictions).keys()) # equals to list(set(words))\n","  print(Counter(predictions).values())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"meT78j8Bbprl","executionInfo":{"status":"ok","timestamp":1707249596718,"user_tz":-60,"elapsed":18,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"d0b1c6eb-acc3-41e1-80cc-53d0806958d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Embeddings: MLP (ReLu + SGD)\n","-----\n","tp: 74\n","Precision: 0.9487179487179487\n","recall: 0.5873015873015873\n","f1: 0.7254901960784315\n","dict_keys([1, 0])\n","dict_values([12525, 78])\n","\n","\n","Embeddings: MLP with threshold (ReLu + SGD)\n","-----\n","tp: 83\n","Precision: 0.9325842696629213\n","recall: 0.6587301587301587\n","f1: 0.7720930232558139\n","dict_keys([1, 0])\n","dict_values([12514, 89])\n","\n","\n","Embeddings: SVC with sigmoid)\n","-----\n","tp: 125\n","Precision: 0.8992805755395683\n","recall: 0.9920634920634921\n","f1: 0.9433962264150942\n","dict_keys([1, 0])\n","dict_values([12464, 139])\n","\n","\n","Embeddings: SVC with RBF\n","-----\n","tp: 22\n","Precision: 1.0\n","recall: 0.1746031746031746\n","f1: 0.29729729729729726\n","dict_keys([1, 0])\n","dict_values([12581, 22])\n","\n","\n","Embeddings Ensemble MLP (ReLu + SGD)\n","-----\n","tp: 124\n","Precision: 0.6458333333333334\n","recall: 0.9841269841269841\n","f1: 0.779874213836478\n","dict_keys([1, 0])\n","dict_values([12411, 192])\n","\n","\n","Embeddings: Ensemble MLP and SVC\n","-----\n","tp: 125\n","Precision: 0.6510416666666666\n","recall: 0.9920634920634921\n","f1: 0.7861635220125787\n","dict_keys([1, 0])\n","dict_values([12411, 192])\n"]}]},{"cell_type":"code","source":["with open(path_store_results + f\"{RESULTS_FILE}_external_{TYPE_EXTERNAL_POLLUTION}.pkl\" , 'wb') as file:\n","  # Writing the dictionary to the pickle file\n","  pickle.dump(log_external, file)"],"metadata":{"id":"oCYUMZv2sV1b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prompt engineering"],"metadata":{"id":"QIURns3IRkh4"}},{"cell_type":"code","source":["def clip_generated_label(img_encoding, text_features_normalized):\n","  # source: https://github.com/openai/CLIP\n","\n","\n","  similarity = (100.0 * img_encoding @ text_features_normalized.T).softmax(dim=-1)\n","  reducer = lambda x: 1 if x[0]>0.5 else 0\n","  return [reducer(x) for x in similarity]"],"metadata":{"id":"Rqwvs_C0RjlW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_inputs = [f\"a photo of a {c}\" for c in [DOG_OR_CAT,\"something\"]]\n","\n","# Calculate features\n","text_features=[]\n","for i in text_inputs:\n","  with torch.no_grad():\n","    text_features.append(model.encode_text(tokenize(i).to(device)).cpu())\n","\n","text_features=torch.cat(text_features, 0)\n","images = image_embeddings[DOG_OR_CAT]\n","# Pick the top 5 most similar labels for the image\n","img_encoding = images/ images.norm(dim=-1, keepdim=True)\n","text_features /= text_features.norm(dim=-1, keepdim=True)\n","\n","predicted = clip_generated_label(img_encoding, text_features)\n","\n","\n","calculate_metrics(predicted,  y_test_embeddings_internal)\n","print(Counter(predicted).keys())\n","print(Counter(predicted).values())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sNqBxWRGSHv2","executionInfo":{"status":"ok","timestamp":1707249597272,"user_tz":-60,"elapsed":570,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"68bdf2eb-8ff0-4cf3-8eb2-2bb995fb9408"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-----\n","tp: 13\n","Precision: 0.65\n","recall: 0.52\n","f1: 0.5777777777777778\n","dict_keys([1, 0])\n","dict_values([12482, 20])\n"]}]}],"metadata":{"colab":{"collapsed_sections":["-8RYGhGcVQp7","lkIgVhzGhreu","aUki7WXfjgwC","SMoCzhM7Cpva","vUZ7W2lg0s7d","fC03u_O5eOp6","nR72nR0GFe3j","FQsWA_vlRGE0","nQMRryl9RKHL","7LfPoF11SvOl","9kqo5bDkshsq"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}