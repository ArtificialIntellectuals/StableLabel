{"cells":[{"cell_type":"markdown","source":["Notebook for experiments on Prompt engineering using CLIP or ALIGN"],"metadata":{"id":"ebVUKVtJ6Uh7"}},{"cell_type":"markdown","metadata":{"id":"M5zokF87VQGp"},"source":["# Choose settings"]},{"cell_type":"markdown","metadata":{"id":"Lu2CK4YAl9qA"},"source":["##### Choose your settings here"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"y4Ef3a0WVPdF","executionInfo":{"status":"ok","timestamp":1707295136000,"user_tz":-60,"elapsed":220,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"outputs":[],"source":["DOG_OR_CAT = \"cat\" #can also be \"dog\"\n","TYPE_EXTERNAL_POLLUTION = \"cross-label\" #other option: \"imagenet_only\". With cross-label: 1% pollution, 1/2 of that cross-label, other 1/2 imagenet\n","MODEL=\"CLIP\"\n","\n","# choose dataset\n","DATASET_NAME = 'cats-vs-dogs-large'  # needs to match folder name in FM/datasets\n","LOAD_AND_EMBED_DATASET_IN_BATCHES = True  # True for large datasets, False for small ones\n","USE_CACHED_EMBEDDINGS = f'{MODEL}_cats-vs-dogs-large.pkl'  # '' for loading the dataset normally, 'CREATE__{x}.pkl' for creating the cache file {x}.pkl, '{x}.pkl' for loading the cache file {x}.pkl\n","MISLABELED_INSTANCES = 'mislabeled_instances_cats-vs-dogs.pkl'  # if not '', but e.g. 'mislabeled_instances_cats-vs-dogs.pkl', the pickle file specifies which files to drop from the loaded embeddings\n","\n","\n","\n","# choose how many image-label mismatches to insert\n","MISMATCH_PORTION = 0.01  # percentage of mismatching image-label pairs added\n","MANIPULATION_TYPES = [0.5, 0.5, 0.0, 0.0]  # how much of the MISMATCH_PORTION to produce by [exchanging images between classes, inserting images from other datasets, inserting randomly generated images, inserting placeholder images]\n","IMAGENET_EMBEDDINGS = f'{MODEL}_imagenet-subset.pkl'  # specify if MANIPULATION_TYPES[1] > 0\n"]},{"cell_type":"markdown","metadata":{"id":"Voe1lmjsl8_c"},"source":["##### This part is calculated automatically"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VRv_RrFZl5pb","executionInfo":{"status":"ok","timestamp":1707293293531,"user_tz":-60,"elapsed":5,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"outputs":[],"source":["datasets_path = '/content/drive/My Drive/FM/datasets/'\n","dataset_path = datasets_path + DATASET_NAME + '/'\n","\n","if DATASET_NAME == 'cats-vs-dogs-large' or DATASET_NAME == 'train-small':\n","  LABELS = ['cat', 'dog']\n","elif DATASET_NAME == 'jellyfish-classification':\n","  LABELS = ['barrel jellyfish', 'compass jellyfish', 'lions mane jellyfish', 'moon jellyfish']\n","elif DATASET_NAME == 'traffic-signs':\n","  LABELS = ['30 kilometers per hour speed limit traffic sign', '80 kilometers per hour speed limit traffic sign', '100 kilometers per hour speed limit traffic sign', 'give way traffic sign', 'no entry traffic sign', 'no overtaking traffic sign', 'priority over oncoming traffic sign', 'stop sign']\n","else:\n","  raise ValueError('Invalid dataset selected or labels not set!')\n","\n","assert 0 <= MISMATCH_PORTION <= 1, f'MISMATCH_PORTION must be in [0, 1] but is {MISMATCH_PORTION}'\n","assert len(MANIPULATION_TYPES) == 4 and sum(MANIPULATION_TYPES) == 1, f'MANIPULATION_TYPES must contain 4 entries that sum up to 1.0 but is {MANIPULATION_TYPES}'\n","assert MANIPULATION_TYPES[0] + MANIPULATION_TYPES[1] == 1, 'At the moment, only interclass corruption and imagenet corruption are implemented!'"]},{"cell_type":"markdown","metadata":{"id":"-8RYGhGcVQp7"},"source":["# Load libraries"]},{"cell_type":"code","source":["# hacky way when hitting \"run all\" that libraries are not reloaded\n","try:\n","  torch.tensor([[0]])\n","  libraries_already_loaded = True\n","except:\n","  libraries_already_loaded = False"],"metadata":{"id":"ouMUhtN7GcGZ","executionInfo":{"status":"ok","timestamp":1707293293531,"user_tz":-60,"elapsed":4,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"nff6q1HOlfh0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707293319933,"user_tz":-60,"elapsed":26136,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"b474d965-6c16-4dd4-c07b-d61d83d698ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m927.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.3\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-3qh4vvyl\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-3qh4vvyl\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=e758d612815ed9fe108e8302b97b3327e437513daf486f67db1edd201be69ec0\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_gzr0gx6/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]}],"source":["if not libraries_already_loaded:\n","  ! pip install ftfy regex tqdm\n","  ! pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1707293388288,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"},"user_tz":-60},"id":"vPsiwNE2nmAB","outputId":"3ee3e4aa-eae6-42b5-c4aa-aa8dee1c5f64"},"outputs":[{"output_type":"stream","name":"stdout","text":["Torch version: 2.1.0+cu121\n"]}],"source":["import torch\n","from torchvision import transforms\n","import clip\n","from transformers import AutoImageProcessor, ViTModel, AlignProcessor, AlignModel, AutoTokenizer\n","from transformers.tokenization_utils_base import BatchEncoding\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","from pkg_resources import packaging\n","import os\n","from google.colab import drive\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n","import glob\n","import pickle\n","from scipy.spatial.distance import cosine\n","\n","print(\"Torch version:\", torch.__version__)"]},{"cell_type":"code","source":["def calculate_column_average(matrix):\n","    \"\"\"\n","    Calculates the average for each column in a PyTorch matrix.\n","\n","    Parameters:\n","    - matrix: torch.Tensor\n","        The input matrix for which the column averages are to be calculated.\n","\n","    Returns:\n","    - torch.Tensor\n","        A tensor containing the average value for each column of the input matrix.\n","\n","    Raises:\n","    - TypeError:\n","        Raises an error if the input is not a PyTorch tensor.\n","    - ValueError:\n","        Raises an error if the input tensor is empty or has zero columns.\n","    \"\"\"\n","\n","    # Checking if the input is a PyTorch tensor\n","    if not isinstance(matrix, torch.Tensor):\n","        raise TypeError(\"Input should be a PyTorch tensor.\")\n","\n","    # Checking if the input tensor is empty or has zero columns\n","    if matrix.numel() == 0 or matrix.size(1) == 0:\n","        raise ValueError(\"Input tensor is empty or has zero columns.\")\n","\n","    # Calculating the column averages but taking out the entry for the same vector in the matrix (diagonal entry)\n","    column_sums = torch.sum(matrix, dim=0)\n","    column_counts = torch.tensor([matrix.size(0)] * (matrix.size(1)), dtype=torch.float32)\n","    column_averages = (column_sums -1) / (column_counts - 1)\n","\n","    return column_averages\n","\n","def cosine_similarity_matrix(embeddings_tensor):\n","    \"\"\"\n","    Function to compute a similarity matrix using dot product\n","\n","    Parameters:\n","    - embeddings: list of torch.Tensor\n","        List of embeddings of images produced with CLIP.\n","\n","    Returns:\n","    - similarity_matrix: numpy.ndarray\n","        2D numpy array representing the similarity matrix between the embeddings.\n","        Each element (i, j) in the matrix represents the similarity between embeddings[i] and embeddings[j].\n","        The similarity score between the two embeddings is calculated using cos similarity. The score ranges from 0 to 1,\n","        where 0 indicates completely dissimilar embeddings and 1 indicates identical embeddings.\n","    \"\"\"\n","\n","    \"\"\"\n","    # Calculating the cosine distance between the two embeddings\n","    distance = cosine(embedding1, embedding2)\n","\n","    # Converting the distance to similarity score\n","    similarity = 1 - distance\n","\n","    return similarity\n","    \"\"\"\n","\n","    # Normalizing the embeddings\n","    embeddings_tensor = torch.nn.functional.normalize(embeddings_tensor, dim=1)\n","\n","    # Computing the similarity matrix using dot product\n","    similarity_matrix = torch.matmul(embeddings_tensor, embeddings_tensor.T)\n","\n","    return similarity_matrix\n","\n","from sklearn.metrics.pairwise import cosine_similarity as cosine_similarity_sklearn\n","def cosine_similarity(x, y=None):\n","  if y is None:\n","    return cosine_similarity_matrix(x)\n","  return cosine_similarity_sklearn(x, y)"],"metadata":{"id":"2OoiHPpeNC7S","executionInfo":{"status":"ok","timestamp":1707293341279,"user_tz":-60,"elapsed":17,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lkIgVhzGhreu"},"source":["# Mounting storage"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28365,"status":"ok","timestamp":1707293369630,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"},"user_tz":-60},"id":"RIOqbAUohxeV","outputId":"54184ce5-5380-4dd4-aa96-3dafc25860a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","AlexNet_cats-vs-dogs-large.pkl\t\t\t mislabeled_instances_cats-vs-dogs.pkl\n","AlexNet_imagenet-subset.pkl\t\t\t note.txt\n","AlexNet_traffic-signs.pkl\t\t\t text_dog_embeddings.pkl\n","ALIGN_cats-vs-dogs-large.pkl\t\t\t text_embeddings_correct_align_cat.pkl\n","ALIGN_imagenet-subset.pkl\t\t\t text_embeddings_correct_align_dog.pkl\n","ALIGN_traffic-signs.pkl\t\t\t\t text_embeddings_correct_clip_cat.pkl\n","cats-dogs-big_ids.pkl\t\t\t\t text_embeddings_correct_clip_dog.pkl\n","cats-dogs-big.pkl\t\t\t\t text_embeddings_random_align_cat.pkl\n","cats-vs-dogs-large\t\t\t\t text_embeddings_random_align_dog.pkl\n","CLIP_cats-vs-dogs-large.pkl\t\t\t text_embeddings_random_clip_cat.pkl\n","CLIP_imagenet-subset.pkl\t\t\t text_embeddings_random_clip_dog.pkl\n","CLIP_traffic-signs.pkl\t\t\t\t text_random_embeddings.pkl\n","dog_wrong_2_12.txt\t\t\t\t traffic-signs\n","dog_wrong.txt\t\t\t\t\t train-small\n","image_embeddings__cats-vs-dogs.pkl\t\t ViT-CLS_cats-vs-dogs-large.pkl\n","image_embeddings__traffic-signs.pkl\t\t ViT-CLS_imagenet-subset.pkl\n","imagenet_one-of-each-class-except-cats-and-dogs  ViT-CLS_traffic-signs.pkl\n","indizes_clean_Anni.txt\t\t\t\t ViT-pooling_cats-vs-dogs-large.pkl\n","jellyfish-classification\t\t\t ViT-pooling_imagenet-subset.pkl\n","mislabeled_instances_cats-vs-dogs_CATS_ONLY.pkl  ViT-pooling_traffic-signs.pkl\n"]}],"source":["drive.mount('/content/drive')\n","!ls \"{datasets_path}\""]},{"cell_type":"markdown","metadata":{"id":"aUki7WXfjgwC"},"source":["# Define dataset loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAu1VUDTjw1t","executionInfo":{"status":"aborted","timestamp":1707293374528,"user_tz":-60,"elapsed":20,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"outputs":[],"source":["def load_dataset(folder_path, labels):\n","\n","    # Checking if the provided folder path exists\n","    if not os.path.exists(folder_path):\n","        raise ValueError(\"Folder path does not exist.\")\n","\n","    images = {}\n","    for label in labels:\n","      images[label] = []\n","\n","    # Looping through all files in the folder\n","    for i, filename in enumerate(glob.glob(folder_path + '**/*', recursive=True)):\n","\n","      if i % 1000 == 0:\n","        print(i, 'files loaded')\n","\n","      try:\n","        img = Image.open(filename).convert('RGB')\n","      except:\n","        continue\n","\n","      label_found = False\n","      for label in labels:\n","        if label in '/'.join(filename.split('/')[-2:]):\n","          if label_found:\n","            raise ValueError(f\"Label of {filename} is ambiguous.\")\n","          label_found = True\n","          images[label].append(img)\n","\n","      if not label_found:\n","        raise ValueError(f\"No label for {filename} found.\")\n","\n","    print(i+1, 'files loaded')\n","\n","    return images\n","\n","def get_embeddings_dict_batchwise(folder_path, labels, model, preprocess, batch_size=64):\n","\n","  # Checking if the provided folder path exists\n","  if not os.path.exists(folder_path):\n","    raise ValueError(\"Folder path does not exist.\")\n","\n","  image_embeddings = {}\n","  for label in labels:\n","    image_embeddings[label] = []\n","\n","  images = {}\n","  for label in labels:\n","    images[label] = []\n","\n","  # Looping through all files in the folder\n","  all_files = glob.glob(folder_path + '**/*', recursive=True)\n","  for n_instances_processed, filename in enumerate(all_files):\n","\n","    try:\n","      img = Image.open(filename).convert('RGB')\n","    except:\n","      continue\n","\n","    # Find label of the image\n","    label_found = False\n","    for label in labels:\n","      if label in '/'.join(filename.split('/')[-2:]):\n","        if label_found:\n","          raise ValueError(f\"Label of {filename} is ambiguous.\")\n","        label_found = True\n","        images[label].append(img)\n","    if not label_found:\n","      raise ValueError(f\"No label for {filename} found.\")\n","\n","    # Get embeddings if already a batch is full\n","    if n_instances_processed % batch_size == 0 and n_instances_processed > 0 or n_instances_processed == len(all_files) - 1:\n","      for label in labels:\n","        if len(images[label]) == 0:\n","          continue\n","        with torch.no_grad():\n","          processed_images = torch.cat(([preprocess(img).unsqueeze(0) for img in images[label]]))\n","          image_embeddings[label].append(model.encode_image(processed_images.to(device)))\n","          del processed_images\n","      images = {}\n","      for label in labels:\n","        images[label] = []\n","      print(n_instances_processed, 'loaded and encoded')\n","\n","  # Convert list of embeddings to tensor\n","  for label in labels:\n","    image_embeddings[label] = torch.cat((image_embeddings[label]))\n","\n","  return image_embeddings"]},{"cell_type":"markdown","metadata":{"id":"SMoCzhM7Cpva"},"source":["# Load and embed dataset"]},{"cell_type":"code","source":["if USE_CACHED_EMBEDDINGS != '' and USE_CACHED_EMBEDDINGS.split('__')[0] != 'CREATE':\n","\n","  # load embeddings of previous execution from pickle file\n","  pickle_file = datasets_path + USE_CACHED_EMBEDDINGS\n","  with open(pickle_file, 'rb') as f:\n","    image_embeddings = pickle.load(f)\n","\n","  print('Embeddings loaded from', pickle_file)\n","\n","else:\n","\n","  if LOAD_AND_EMBED_DATASET_IN_BATCHES:\n","\n","    # load and embed images in batches (to save GPU memory and especially RAM)\n","    image_embeddings = get_embeddings_dict_batchwise(dataset_path, LABELS, model, preprocess, batch_size=512)\n","\n","  else:\n","\n","    # load images\n","    images = load_dataset(dataset_path, LABELS)\n","\n","    # embed images and text\n","    image_embeddings = {}\n","    for label in LABELS:\n","      processed_images = torch.cat(([preprocess(img).unsqueeze(0) for img in images[label]])).to(device)\n","      with torch.no_grad():\n","        image_embeddings[label] = model.encode_image(processed_images)\n","\n","  # move embeddings to cpu and convert to suitable datatype for further analysis\n","  for key in image_embeddings:\n","    image_embeddings[key] = image_embeddings[key].cpu().type(torch.float)\n","\n","  # save embeddings in pickle file if desired (enabled to reload them later on)\n","  if USE_CACHED_EMBEDDINGS != '' and USE_CACHED_EMBEDDINGS.split('__')[0] == 'CREATE':\n","    pickle_filename = '__'.join(USE_CACHED_EMBEDDINGS.split('__')[1:])  # remove prefix 'CREATE__'\n","    pickle_file = datasets_path + pickle_filename\n","    with open(pickle_file, 'wb') as f:\n","      pickle.dump(image_embeddings, f)\n","    print('Embeddings stored in', pickle_file)\n","\n","# save embedding dimension for creation of reference vectors etc.\n","embedding_dim = image_embeddings[LABELS[0]].shape[1]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OM6pCR1ibfGv","executionInfo":{"status":"ok","timestamp":1707295028393,"user_tz":-60,"elapsed":233,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"307ce4ba-b3c1-4906-826a-ad4def5b2a58"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings loaded from /content/drive/My Drive/FM/datasets/CLIP_cats-vs-dogs-large.pkl\n"]}]},{"cell_type":"code","source":["LABELS_POLLUTION_ALSO = LABELS.copy()\n","mislabeled_indices = None\n","\n","if MISLABELED_INSTANCES != '':\n","  with open(datasets_path + MISLABELED_INSTANCES, 'rb') as f:\n","    mislabeled_indices = pickle.load(f)\n","  for label in mislabeled_indices:\n","    if len(mislabeled_indices[label]) == 0 or sum(mislabeled_indices[label]) == 0:\n","      continue\n","    image_embeddings[label + '_pollution'] = image_embeddings[label][mislabeled_indices[label]].clone().detach()\n","    image_embeddings[label + '_clean'] = image_embeddings[label][[not i for i in mislabeled_indices[label]]].clone().detach()\n","    LABELS_POLLUTION_ALSO += [label + '_pollution', label + '_clean']\n","\n","if MANIPULATION_TYPES[1] > 0:\n","  pickle_file = datasets_path + IMAGENET_EMBEDDINGS\n","  with open(pickle_file, 'rb') as f:\n","    image_embeddings['imagenet_subset'] = pickle.load(f)['val']\n","  print('Embeddings loaded from', pickle_file)\n","  LABELS_POLLUTION_ALSO += ['imagenet_subset']\n","\n","print('LABELS =', LABELS)\n","print('LABELS_POLLUTION_ALSO =', LABELS_POLLUTION_ALSO)\n","for k in image_embeddings:\n","  print(k, len(image_embeddings[k]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfSc3hCv1A3E","executionInfo":{"status":"ok","timestamp":1707295028658,"user_tz":-60,"elapsed":9,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"f6dfd933-d6fb-4a94-ed49-d693096b1a34"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings loaded from /content/drive/My Drive/FM/datasets/CLIP_imagenet-subset.pkl\n","LABELS = ['cat', 'dog']\n","LABELS_POLLUTION_ALSO = ['cat', 'dog', 'cat_pollution', 'cat_clean', 'dog_pollution', 'dog_clean', 'imagenet_subset']\n","cat 12502\n","dog 12499\n","cat_pollution 25\n","cat_clean 12477\n","dog_pollution 24\n","dog_clean 12475\n","imagenet_subset 869\n"]}]},{"cell_type":"code","source":["MISLABELED_INSTANCES_LIST = 'mislabeled_instances_cats-vs-dogs.pkl'\n","\n","with open(datasets_path + MISLABELED_INSTANCES_LIST, 'rb') as f:\n","  mislabeled_indices = pickle.load(f)"],"metadata":{"id":"AbQEPAC8TtMo","executionInfo":{"status":"ok","timestamp":1707295028658,"user_tz":-60,"elapsed":7,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["imagenet_arr = [ image_embeddings[\"imagenet_subset\"][i,:].numpy() for i in range(image_embeddings[\"imagenet_subset\"].shape[0])]"],"metadata":{"id":"zNU1BMKeawKK","executionInfo":{"status":"ok","timestamp":1707295028929,"user_tz":-60,"elapsed":1,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":["# Model\n","\n"],"metadata":{"id":"j5A0exVHcS8B"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","\n","class Align(torch.nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.align = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    return self.align(x)\n","\n","  def encode_image(self, img: torch.Tensor) -> torch.Tensor:\n","    return self.align.get_image_features(img)\n","\n","  def encode_text(self, text: BatchEncoding) -> torch.Tensor:\n","    return self.align.get_text_features(**text)\n","\n","\n","def align_preprocessor_with_memory_fix(img) -> torch.Tensor:\n","  processor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\n","  with torch.no_grad():\n","    processed = processor(images=img, return_tensors=\"pt\").to(device).pixel_values.squeeze(0)\n","  del processor\n","  return processed\n","\n","\n","if MODEL == 'CLIP':\n","  model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","  tokenize = clip.tokenize\n","elif MODEL == 'ALIGN':\n","  model = Align().to(device)\n","  preprocess = align_preprocessor_with_memory_fix\n","  tokenizer = AutoTokenizer.from_pretrained(\"kakaobrain/align-base\")\n","  tokenize = lambda s: tokenizer([s], padding=True, return_tensors=\"pt\")\n","else:\n","  raise ValueError(f'Invalid model {MODEL} selected!')"],"metadata":{"id":"uTFZo88OV88Z","executionInfo":{"status":"ok","timestamp":1707295038051,"user_tz":-60,"elapsed":6208,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"d10SX9zicVRs"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","def calculate_metrics(prediction, ground_truth):\n","\n","    # Checking if the lengths of the prediction and ground truth arrays are equal.\n","    if len(prediction) != len(ground_truth):\n","        raise ValueError(\"Lengths of prediction and ground truth arrays should be equal.\")\n","\n","    tp = 0\n","    tn = 0\n","    fp = 0\n","    fn = 0\n","    for pred, truth in zip(prediction, ground_truth):\n","      if pred == truth:\n","        if pred == 0:\n","          tp += 1\n","        else:\n","          tn +=1\n","      elif pred == 0:\n","        fp +=1\n","      else:\n","        fn +=1\n","\n","    if (tp+fp) >0:\n","      prec = tp / (tp+fp)\n","    else:\n","      prec=0\n","    if (tp+fn)>0:\n","      rec = tp / (tp+fn)\n","    else:\n","      rec=0\n","    if prec!= 0 and rec !=0:\n","      f1 = 2* prec* rec/(prec+rec)\n","    else:\n","      f1 = 0\n","\n","    print(\"-----\")\n","    print(f\"tp: {tp}\")\n","    print(f\"Precision: {prec}\")\n","    print(f\"recall: {rec}\")\n","    print(f\"f1: {f1}\")\n","\n","    return {\"tp\":tp,\"p\":prec,\"r\":rec,\"f1\":f1}"],"metadata":{"id":"P4q85Vpzbo87","executionInfo":{"status":"ok","timestamp":1707293427353,"user_tz":-60,"elapsed":213,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Prompt engineering"],"metadata":{"id":"7LQ_rVy2kZr0"}},{"cell_type":"code","source":["def clip_generated_label(img_encoding, text_features_normalized):\n","\n","  similarity = (100.0 * img_encoding @ text_features_normalized.T).softmax(dim=-1)\n","  reducer = lambda x: 1 if x[0]>0.5 else 0\n","  return [reducer(x) for x in similarity]"],"metadata":{"id":"mxsaYsPBkY27","executionInfo":{"status":"ok","timestamp":1707293437874,"user_tz":-60,"elapsed":457,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["ground_truth = []\n","for emb, correct in zip(image_embeddings[DOG_OR_CAT],mislabeled_indices[DOG_OR_CAT]):\n","  if correct == False:\n","    ground_truth.append(1)\n","  else:\n","    ground_truth.append(0)"],"metadata":{"id":"1Xf4aCHjUJRQ","executionInfo":{"status":"ok","timestamp":1707294358288,"user_tz":-60,"elapsed":224,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","prompts = [f\"no {DOG_OR_CAT}\",\"adbadmn\",\"random\",\"something\",\"nothing\",\"strange\",\"other\"]\n","\n","images = image_embeddings[DOG_OR_CAT]\n","# Pick the top 5 most similar labels for the image\n","img_encoding = images/ images.norm(dim=-1, keepdim=True)\n","\n","print(MODEL)\n","for x in prompts:\n","  text_inputs = [f\"a photo of a {c}\" for c in [DOG_OR_CAT,x]]\n","\n","  # Calculate features\n","  text_features=[]\n","  for i in text_inputs:\n","    with torch.no_grad():\n","      text_features.append(model.encode_text(tokenize(i).to(device)).cpu())\n","\n","  text_features=torch.cat(text_features, 0)\n","  text_features /= text_features.norm(dim=-1, keepdim=True)\n","\n","  predicted = clip_generated_label(img_encoding, text_features)\n","\n","  print(x)\n","  calculate_metrics(predicted,  ground_truth)\n","  print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20LYqt6DmanU","executionInfo":{"status":"ok","timestamp":1707294737409,"user_tz":-60,"elapsed":5182,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"36342184-3996-4b55-ade4-9f70af36b032"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["ALIGN\n","no cat\n","-----\n","tp: 10\n","Precision: 0.008960573476702509\n","recall: 0.4\n","f1: 0.017528483786152498\n","\n","adbadmn\n","-----\n","tp: 11\n","Precision: 0.6111111111111112\n","recall: 0.44\n","f1: 0.5116279069767442\n","\n","random\n","-----\n","tp: 10\n","Precision: 0.4166666666666667\n","recall: 0.4\n","f1: 0.4081632653061225\n","\n","something\n","-----\n","tp: 10\n","Precision: 0.6666666666666666\n","recall: 0.4\n","f1: 0.5\n","\n","nothing\n","-----\n","tp: 8\n","Precision: 0.6153846153846154\n","recall: 0.32\n","f1: 0.4210526315789474\n","\n","strange\n","-----\n","tp: 7\n","Precision: 0.28\n","recall: 0.28\n","f1: 0.28\n","\n","other\n","-----\n","tp: 12\n","Precision: 0.7058823529411765\n","recall: 0.48\n","f1: 0.5714285714285713\n","\n"]}]},{"cell_type":"markdown","source":["## On external pollution - label, \"something\""],"metadata":{"id":"_f_f-NVIe3zg"}},{"cell_type":"code","source":["if TYPE_EXTERNAL_POLLUTION == \"cross-label\":\n","  if DOG_OR_CAT ==\"cat\":\n","    other_label = \"dog\"\n","  else:\n","    other_label = \"cat\"\n","\n","  size_clean = len(image_embeddings[f\"{DOG_OR_CAT}_clean\"].tolist())\n","\n","  images = image_embeddings[f\"{DOG_OR_CAT}_clean\"].tolist() + imagenet_arr[:int(size_clean/198)] + image_embeddings[other_label][:int(size_clean/198)].tolist()\n","  golden = [1 for i in range(size_clean)] + [0 for i in range(int(size_clean/99))]\n","\n","else:\n","  images = image_embeddings[f\"{DOG_OR_CAT}_clean\"].tolist() + imagenet_arr\n","  golden = [1 for i in range(len(image_embeddings[f\"{DOG_OR_CAT}_clean\"]))] + [0 for i in range(len(imagenet_arr))]\n","\n","text_inputs = [f\"a photo of a {c}\" for c in [DOG_OR_CAT,\"Something\"]]\n","\n","# Calculate features\n","text_features=[]\n","for i in text_inputs:\n","  with torch.no_grad():\n","    text_features.append(model.encode_text(tokenize(i).to(device)).cpu())\n","\n","text_features=torch.cat(text_features, 0)\n","images = torch.Tensor(images)\n","# Pick the top 5 most similar labels for the image\n","img_encoding = images/ images.norm(dim=-1, keepdim=True)\n","text_features /= text_features.norm(dim=-1, keepdim=True)\n","\n","predicted = clip_generated_label(img_encoding, text_features)"],"metadata":{"id":"VYhLYVccIBl0","executionInfo":{"status":"ok","timestamp":1707295144323,"user_tz":-60,"elapsed":2167,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["len(predicted)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-b6YRNYIbCx","executionInfo":{"status":"ok","timestamp":1707295144323,"user_tz":-60,"elapsed":12,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"a287ca18-f15d-43c6-e1d8-24be3b736e0b"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12603"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["from collections import Counter\n","print(f\"model: {MODEL}\")\n","print(f\"dataset: {DOG_OR_CAT}\")\n","calculate_metrics(predicted,  golden)\n","#print(Counter(predicted).keys())\n","#print(Counter(predicted).values())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pfb4wyzIO4p","executionInfo":{"status":"ok","timestamp":1707295144323,"user_tz":-60,"elapsed":9,"user":{"displayName":"Anna-Maria Halacheva","userId":"17079454886064192026"}},"outputId":"89b0f967-df53-428f-e0fd-e34a8a0ee36c"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["model: CLIP\n","dataset: cat\n","-----\n","tp: 125\n","Precision: 0.946969696969697\n","recall: 0.9920634920634921\n","f1: 0.9689922480620156\n"]},{"output_type":"execute_result","data":{"text/plain":["{'tp': 125,\n"," 'p': 0.946969696969697,\n"," 'r': 0.9920634920634921,\n"," 'f1': 0.9689922480620156}"]},"metadata":{},"execution_count":69}]}],"metadata":{"colab":{"collapsed_sections":["M5zokF87VQGp","-8RYGhGcVQp7","lkIgVhzGhreu","aUki7WXfjgwC","SMoCzhM7Cpva","d10SX9zicVRs","2k6oIJXfejhG","U79RL6zNeopB","xmsTTDPmfUEv","AGyNzTQZfd5a","xN1NtwJpfh60","x51pwnC4fnZB","M8X-tu8Rf0ul"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}