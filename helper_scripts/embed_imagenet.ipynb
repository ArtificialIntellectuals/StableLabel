{"cells":[{"cell_type":"markdown","metadata":{"id":"M5zokF87VQGp"},"source":["# Choose settings"]},{"cell_type":"markdown","metadata":{"id":"Lu2CK4YAl9qA"},"source":["##### Choose your settings here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y4Ef3a0WVPdF"},"outputs":[],"source":["# choose dataset\n","DATASET_NAME = 'imagenet_one-of-each-class-except-cats-and-dogs'  # needs to match folder name in FM/datasets\n","LOAD_AND_EMBED_DATASET_IN_BATCHES = False  # True for large datasets, False for small ones\n","USE_CACHED_EMBEDDINGS = 'CREATE__ViT-CLS_imagenet-subset.pkl'  # '' for loading the dataset normally, 'CREATE__{x}.pkl' for creating the cache file {x}.pkl, '{x}.pkl' for loading the cache file {x}.pkl\n","\n","# choose which model to use to generate the embeddings\n","MODEL = 'ViT-CLS'  # Can be 'CLIP', 'AlexNet', 'ViT-pooling', 'ViT-CLS' or 'ALIGN'"]},{"cell_type":"markdown","metadata":{"id":"Voe1lmjsl8_c"},"source":["##### This part is calculated automatically"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRv_RrFZl5pb"},"outputs":[],"source":["datasets_path = '/content/drive/My Drive/FM/datasets/'\n","dataset_path = datasets_path + DATASET_NAME + '/'\n","\n","if DATASET_NAME == 'cats-vs-dogs-large' or DATASET_NAME == 'train-small':\n","  LABELS = ['cat', 'dog']\n","elif DATASET_NAME == 'jellyfish-classification':\n","  LABELS = ['barrel jellyfish', 'compass jellyfish', 'lions mane jellyfish', 'moon jellyfish']\n","elif DATASET_NAME == 'traffic-signs':\n","  LABELS = ['30 kilometers per hour speed limit traffic sign', '80 kilometers per hour speed limit traffic sign', '100 kilometers per hour speed limit traffic sign', 'give way traffic sign', 'no entry traffic sign', 'no overtaking traffic sign', 'priority over oncoming traffic sign', 'stop sign']\n","elif DATASET_NAME == 'imagenet_one-of-each-class-except-cats-and-dogs':\n","  LABELS = ['val']\n","else:\n","  raise ValueError('Invalid dataset selected or labels not set!')\n","\n","assert MODEL in ['CLIP', 'AlexNet', 'ViT-pooling', 'ViT-CLS', 'ALIGN'], f'Selected model {MODEL} not implemented!'"]},{"cell_type":"markdown","metadata":{"id":"-8RYGhGcVQp7"},"source":["# Load libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16808,"status":"ok","timestamp":1701007234431,"user":{"displayName":"Manuel Schwartz","userId":"07250716813509909308"},"user_tz":-60},"id":"nff6q1HOlfh0","outputId":"d33f3885-c962-4bab-aed0-c384f36f1180"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.1.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.12)\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-1tckegpk\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-1tckegpk\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"]}],"source":["! pip install ftfy regex tqdm\n","! pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1701007234432,"user":{"displayName":"Manuel Schwartz","userId":"07250716813509909308"},"user_tz":-60},"id":"vPsiwNE2nmAB","outputId":"3d7e0df0-33c0-4ff9-ab4a-448d05e310b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Torch version: 2.1.0+cu118\n"]}],"source":["import torch\n","from torchvision import transforms\n","import clip\n","from transformers import AutoImageProcessor, ViTModel, AlignProcessor, AlignModel, AutoTokenizer\n","from transformers.tokenization_utils_base import BatchEncoding\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","from pkg_resources import packaging\n","import os\n","from google.colab import drive\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n","import glob\n","import pickle\n","from scipy.spatial.distance import cosine\n","\n","print(\"Torch version:\", torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"3BxP7PfZn2eg"},"source":["# Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":695,"status":"ok","timestamp":1701007235118,"user":{"displayName":"Manuel Schwartz","userId":"07250716813509909308"},"user_tz":-60},"id":"X5sAGwfJn5fc","outputId":"0f94a29e-a6ee-4f72-d822-c065c8ec44df"},"outputs":[{"output_type":"stream","name":"stdout","text":["VisionTransformer(\n","  (vit): ViTModel(\n","    (embeddings): ViTEmbeddings(\n","      (patch_embeddings): ViTPatchEmbeddings(\n","        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      )\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): ViTEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (pooler): ViTPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n",")\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","class AlexNetEmbedder(torch.nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.alexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    x = self.alexnet.features(x)\n","    x = self.alexnet.avgpool(x)\n","    return self.alexnet.classifier[:5](torch.flatten(x, 1))\n","\n","  def encode_image(self, img: torch.Tensor) -> torch.Tensor:\n","    return self(img)\n","\n","\n","class VisionTransformer(torch.nn.Module):\n","\n","  def __init__(self, use_pooler_output_instead_of_last_hidden_state=False):\n","    super().__init__()\n","    self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n","    self.use_pooler_output = use_pooler_output_instead_of_last_hidden_state\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    out = self.vit(x, return_dict=True)\n","    if self.use_pooler_output:\n","      return out.pooler_output\n","    else:\n","      return out.last_hidden_state[:, 0, :]\n","\n","  def encode_image(self, img: torch.Tensor) -> torch.Tensor:\n","    return self(img)\n","\n","\n","def vit_preprocessor_with_memory_fix(img) -> torch.Tensor:\n","  processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n","  with torch.no_grad():\n","    processed = processor(img, return_tensors=\"pt\").to(device).pixel_values.squeeze(0)\n","  del processor\n","  return processed\n","\n","\n","class Align(torch.nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.align = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    return self.align(x)\n","\n","  def encode_image(self, img: torch.Tensor) -> torch.Tensor:\n","    return self.align.get_image_features(img)\n","\n","  def encode_text(self, text: BatchEncoding) -> torch.Tensor:\n","    return self.align.get_text_features(**text)\n","\n","\n","def align_preprocessor_with_memory_fix(img) -> torch.Tensor:\n","  processor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\n","  with torch.no_grad():\n","    processed = processor(images=img, return_tensors=\"pt\").to(device).pixel_values.squeeze(0)\n","  del processor\n","  return processed\n","\n","\n","if MODEL == 'CLIP':\n","  model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","  tokenize = clip.tokenize\n","elif MODEL == 'AlexNet':\n","  model = AlexNetEmbedder().to(device)\n","  preprocess = transforms.Compose([\n","      transforms.Resize(256),\n","      transforms.CenterCrop(224),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","  ])\n","elif MODEL[:3] == 'ViT':\n","  model = VisionTransformer(MODEL[4:] == 'pooling').to(device)\n","  preprocess = vit_preprocessor_with_memory_fix\n","elif MODEL == 'ALIGN':\n","  model = Align().to(device)\n","  preprocess = align_preprocessor_with_memory_fix\n","  tokenizer = AutoTokenizer.from_pretrained(\"kakaobrain/align-base\")\n","  tokenize = lambda s: tokenizer([s], padding=True, return_tensors=\"pt\")\n","else:\n","  raise ValueError(f'Invalid model {MODEL} selected!')\n","\n","model.eval()\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"lkIgVhzGhreu"},"source":["# Mounting storage"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2647,"status":"ok","timestamp":1701007237757,"user":{"displayName":"Manuel Schwartz","userId":"07250716813509909308"},"user_tz":-60},"id":"RIOqbAUohxeV","outputId":"feb9d160-7039-42b2-b717-0b82364ac12d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","AlexNet_cats-vs-dogs-large.pkl\t       image_embeddings__traffic-signs.pkl\n","AlexNet_imagenet-subset.pkl\t       imagenet_one-of-each-class-except-cats-and-dogs\n","AlexNet_traffic-signs.pkl\t       indizes_clean_Anni.txt\n","buff_ViT-CLS_cats-dogs_upto-12288.pkl  jellyfish-classification\n","buff_ViT-CLS_cats-dogs_upto-12800.pkl  NOT_ALL_DATA__vit-lhs-cls_image_embeddings__cats-vs-dogs.pkl\n","buff_ViT-CLS_cats-dogs_upto-17920.pkl  NOT_ALL_DATA__vit-pool_image_embeddings__cats-vs-dogs.pkl\n","buff_ViT-CLS_cats-dogs_upto-18432.pkl  note.txt\n","cats-dogs-big_ids.pkl\t\t       text_dog_embeddings.pkl\n","cats-dogs-big.pkl\t\t       text_random_embeddings.pkl\n","cats-vs-dogs-large\t\t       traffic-signs\n","CLIP_cats-vs-dogs-large.pkl\t       train-small\n","CLIP_imagenet-subset.pkl\t       ViT-pooling_cats-vs-dogs-large.pkl\n","CLIP_traffic-signs.pkl\t\t       ViT-pooling_imagenet-subset.pkl\n","dog_wrong.txt\t\t\t       ViT-pooling_traffic-signs.pkl\n","image_embeddings__cats-vs-dogs.pkl\n"]}],"source":["drive.mount('/content/drive')\n","!ls \"{datasets_path}\""]},{"cell_type":"markdown","metadata":{"id":"aUki7WXfjgwC"},"source":["# Define dataset loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAu1VUDTjw1t"},"outputs":[],"source":["def load_dataset(folder_path, labels):\n","\n","    # Checking if the provided folder path exists\n","    if not os.path.exists(folder_path):\n","        raise ValueError(\"Folder path does not exist.\")\n","\n","    images = {}\n","    for label in labels:\n","      images[label] = []\n","\n","    # Looping through all files in the folder\n","    for i, filename in enumerate(glob.glob(folder_path + '**/*', recursive=True)):\n","\n","      if i % 1000 == 0:\n","        print(i, 'files loaded')\n","\n","      try:\n","        img = Image.open(filename).convert('RGB')\n","      except:\n","        continue\n","\n","      label_found = False\n","      for label in labels:\n","        if label in '/'.join(filename.split('/')[-2:]):\n","          if label_found:\n","            raise ValueError(f\"Label of {filename} is ambiguous.\")\n","          label_found = True\n","          images[label].append(img)\n","\n","      if not label_found:\n","        raise ValueError(f\"No label for {filename} found.\")\n","\n","    print(i+1, 'files loaded')\n","\n","    return images\n","\n","def get_embeddings_dict_batchwise(folder_path, labels, model, preprocess, batch_size=64):\n","\n","  # Checking if the provided folder path exists\n","  if not os.path.exists(folder_path):\n","    raise ValueError(\"Folder path does not exist.\")\n","\n","  image_embeddings = {}\n","  for label in labels:\n","    image_embeddings[label] = []\n","\n","  images = {}\n","  for label in labels:\n","    images[label] = []\n","\n","  # Looping through all files in the folder\n","  all_files = glob.glob(folder_path + '**/*', recursive=True)\n","  for n_instances_processed, filename in enumerate(all_files):\n","\n","    try:\n","      img = Image.open(filename).convert('RGB')\n","    except:\n","      continue\n","\n","    # Find label of the image\n","    label_found = False\n","    for label in labels:\n","      if label in '/'.join(filename.split('/')[-2:]):\n","        if label_found:\n","          raise ValueError(f\"Label of {filename} is ambiguous.\")\n","        label_found = True\n","        images[label].append(img)\n","    if not label_found:\n","      raise ValueError(f\"No label for {filename} found.\")\n","\n","    # Get embeddings if already a batch is full\n","    if n_instances_processed % batch_size == 0 and n_instances_processed > 0 or n_instances_processed == len(all_files) - 1:\n","      for label in labels:\n","        if len(images[label]) == 0:\n","          continue\n","        with torch.no_grad():\n","          processed_images = torch.cat(([preprocess(img).unsqueeze(0) for img in images[label]]))\n","          image_embeddings[label].append(model.encode_image(processed_images.to(device)))\n","          del processed_images\n","      images = {}\n","      for label in labels:\n","        images[label] = []\n","      print(n_instances_processed, 'loaded and encoded')\n","\n","  # Convert list of embeddings to tensor\n","  for label in labels:\n","    image_embeddings[label] = torch.cat((image_embeddings[label]))\n","\n","  return image_embeddings"]},{"cell_type":"markdown","metadata":{"id":"SMoCzhM7Cpva"},"source":["# Load and embed dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":207557,"status":"ok","timestamp":1701007445309,"user":{"displayName":"Manuel Schwartz","userId":"07250716813509909308"},"user_tz":-60},"id":"Y4o1_ux4Bzx7","outputId":"2119f26f-370c-46c0-9256-e4dc4bc2abc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 files loaded\n","877 files loaded\n","Embeddings stored in /content/drive/My Drive/FM/datasets/ViT-CLS_imagenet-subset.pkl\n"]}],"source":["if USE_CACHED_EMBEDDINGS != '' and USE_CACHED_EMBEDDINGS.split('__')[0] != 'CREATE':\n","\n","  # load embeddings of previous execution from pickle file\n","  pickle_file = datasets_path + USE_CACHED_EMBEDDINGS\n","  with open(pickle_file, 'rb') as f:\n","    image_embeddings = pickle.load(f)\n","\n","  print('Embeddings loaded from', pickle_file)\n","\n","else:\n","\n","  if LOAD_AND_EMBED_DATASET_IN_BATCHES:\n","\n","    # load and embed images in batches (to save GPU memory and especially RAM)\n","    image_embeddings = get_embeddings_dict_batchwise(dataset_path, LABELS, model, preprocess, batch_size=512)\n","\n","  else:\n","\n","    # load images\n","    images = load_dataset(dataset_path, LABELS)\n","\n","    # embed images and text\n","    image_embeddings = {}\n","    for label in LABELS:\n","      processed_images = torch.cat(([preprocess(img).unsqueeze(0) for img in images[label]])).to(device)\n","      with torch.no_grad():\n","        image_embeddings[label] = model.encode_image(processed_images)\n","\n","  # move embeddings to cpu and convert to suitable datatype for further analysis\n","  for key in image_embeddings:\n","    image_embeddings[key] = image_embeddings[key].cpu().type(torch.float)\n","\n","  # save embeddings in pickle file if desired (enabled to reload them later on)\n","  if USE_CACHED_EMBEDDINGS != '' and USE_CACHED_EMBEDDINGS.split('__')[0] == 'CREATE':\n","    pickle_filename = '__'.join(USE_CACHED_EMBEDDINGS.split('__')[1:])  # remove prefix 'CREATE__'\n","    pickle_file = datasets_path + pickle_filename\n","    with open(pickle_file, 'wb') as f:\n","      pickle.dump(image_embeddings, f)\n","    print('Embeddings stored in', pickle_file)"]}],"metadata":{"colab":{"collapsed_sections":["lkIgVhzGhreu"],"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}